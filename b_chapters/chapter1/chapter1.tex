
\chapter{ACADEMIC PERFORMANCE PREDICTION AND EXPLAINABLE MACHINE LEARNING}
\label{chap:litreview}

Chapter 1 of this bachelor thesis lays out the theoretical and methodological basis of this research. The purpose of Chapter 1 is to present a logical and coherent synthesis of key scientific ideas, analytical models and computational paradigms that are at the core of explainable machine-learning solutions for prediction of students’ academic performance based on recent public datasets. In particular, the theoretical background of this work is built upon a diverse but internally consistent set of perspectives on educational data mining, supervised machine learning and explainable artificial intelligence. Together, this body of scientific theory justifies and, where possible, precedes the empirical analyses carried out in later chapters of this thesis, while positioning these research activities firmly within the landscape of current and leading-edge developments in data analytics and information systems. The prediction of students’ academic performance is one of the major research focuses within the fields of learning analytics and educational data mining because of its practical importance for academic quality management, retention and evidence-based policy. The central theoretical assumption of academic performance modelling is that a student’s educational outcome is, to a significant degree, predictable from past and contextual data about the student’s academic trajectory, background and learning environment. By contrast to purely descriptive approaches to educational research, predictive data analytics seek to formalise and, crucially, generalise relationships between these phenomena for purposes of actionable foresight and early intervention (Romero \& Ventura, 2020) \cite{romero2020}.

The distinction between educational and business or industrial data lies in the fact that student performance is the result of complex socio-cognitive rather than mechanical, biological or transactional processes. Students’ educational outcomes are driven by a mixture of academic, behavioural, demographic and institutional factors that interact and determine students’ success in non-linear and context-sensitive ways. Constructs such as prior academic performance, attendance, engagement, socio-economic status and instruction have all been found to affect student performance and learning, although not necessarily with the same relative importance across all educational levels and contexts (Siemens \& Baker, 2020) \cite{siemens2020}. Consequently, educational predictive analytics call for analytic methods that can capture both additive main effects and higher-order interactions. The dominant data structure for student performance from the perspective of data analysis is that of a tabular, cross-sectional or longitudinal data set, where each row is an individual student or student-course combination described by a vector of features and outcome labels. In contrast to time-series forecasting where the temporal order of observations is the primary organising principle, educational data often combine static (demographics) and dynamic (academic history, learning activity) feature types and allow for varying degrees of correlation between modelled variables (Chao et al., 2025) \cite{chao2025}.

Historically, the academic performance prediction problem has been addressed using classical statistical methods such as linear and logistic regression. The reason is, first, the comparatively small data requirements of these methods compared to contemporary machine-learning algorithms and, second, their strong interpretability that, in educational environments, is crucial for trust and user experience. Logistic regression has been used to model pass/fail or continue/no-continue type of academic outcomes in particular as the conditional probability of a student’s success as a function of explanatory features. Its transparent parameterisation and the availability of a full inferential statistical framework make logistic regression a popular statistical modelling choice in the context of educational data (Kotsiantis et al., 2004) \cite{kotsiantis2004}. However, the main disadvantage of linear models is their decision boundary, which constrains their ability to accurately fit the complex relationships often observed in contemporary educational data. The situation has, to some extent, changed after 2016 as a result of the increased availability of large-scale and detailed educational data sets from learning management systems, e-testing platforms, administrative records and data-sharing partnerships. Supervised machine-learning models including decision trees, random forests, gradient boosting machines and neural networks have shown superior prediction results in many student performance prediction tasks because they can model complex and non-linear relationships and feature interactions without being explicitly programmed to do so (Alzahrani, 2024) \cite{alzahrani2024}. In particular, ensemble methods have been shown to be highly effective in student performance prediction due to their relative robustness to the noise and high dimensionality of educational data.

Random forest is a classifier that builds a set of decision trees on bootstrapped samples of the original dataset and then aggregates their predictions by averaging or majority voting. By doing so, it not only builds a less overfitted, non-linear prediction boundary across multiple dimensions of student data, but also captures feature interactions implicitly. Gradient boosting methods are a special case of ensemble classifiers that use an additive model with a differentiable loss function to sequentially train models on the residuals of previous models in order to maximise the improvement in overall predictive performance. Empirical evidence in the field of educational prediction suggests that random forest and gradient boosting methods consistently outperform single classifiers on student performance prediction tasks, particularly in the presence of behavioural and engagement data in addition to academic performance features (Akçapınar et al., 2019) \cite{akcapinar2019}. Despite these advantages in predictive accuracy and, to an extent, robustness, machine-learning classifiers do not address one of the fundamental theoretical and practical challenges in educational data mining: that of transparency. These models, especially of the black-box type, do not provide direct insights into their internal logic and decision-making process. This fact limits the scope of their adoption in educational settings where a human user typically requires an explanation of model predictions. Machine learning classifiers and neural networks in particular raise serious issues around the fairness, accountability and potential bias of predictions, especially where the predictions have a high-stakes nature or significant impact on the individual student (Holstein et al., 2022) \cite{holstein2022}.

The challenges discussed in this section have been the impetus for the emergence of a distinct line of research known as explainable artificial intelligence or eXplainable AI for short. Explainable AI represents an effort to make the internal decision logic of complex machine-learning models transparent without compromising their predictive performance. Post-hoc explainability methods in particular have seen wide adoption as practical tools for interpretation, primarily because they can be applied to any classification or prediction algorithm regardless of internal structure (Lundberg \& Lee, 2017) \cite{lundberg2017}. SHAP and LIME are, arguably, two of the most widespread post-hoc explainability methods in applied machine learning today. SHAP is a cooperative game theory approach to explainable AI that attributes a prediction outcome to a set of individual features by calculating so-called Shapley values. In other words, SHAP provides a framework for breaking down a prediction into a sum of individual contributions from features in the student data point. As such, SHAP can be used both for local explanations (i.e. why this particular prediction was made) and global explanations (feature importance across a whole dataset). This approach is highly theoretically justified, particularly in terms of its consistency and additivity properties that allow for easy comparison of features’ contributions across classifiers and data points (Molnar, 2022) \cite{molnar2022}. In the context of educational analytics, SHAP-based explainability can help to determine the exact contribution of prior grades, attendance, engagement and other factors in prediction of academic outcomes in a rigorous and mathematically justified way. LIME is an alternative local model-agnostic explanation framework that uses surrogate models, usually simple linear regressions, to approximate the decision boundary of a complex model in the neighbourhood of a specific instance. To this end, LIME perturbs input features around an instance of interest and then measures the changes in prediction. As a result, the method produces a localised and human-readable explanation of which features are most important for the specific student. While not sharing the same consistency guarantees as SHAP, LIME has been used as an explanation tool for individual-level analysis and personalised feedback in educational applications (Ribeiro et al., 2016) \cite{ribeiro2016}.

The concept of explainable AI in educational data mining is directly related to the general goals of learning analytics as a field. Learning analytics does not merely emphasise prediction accuracy as the sole value of educational prediction, but also utility, interpretability and pedagogical significance of modelling results. Predictive models that can be interpreted and explained to a human user can be used to power early-warning systems, provide diagnostic insights to targeted intervention or support and, generally, build trust in computational approaches to academic decision-making. Empirical studies have shown that in the context of institutional decision-support systems, educators are more likely to adopt predictive models that provide explanations for their outputs than their black-box counterparts of comparable performance (Altabrawee et al., 2019) \cite{altabrawee2019}. The present thesis also relies on the technological maturity of contemporary data-science and machine-learning ecosystems. The Python programming language has emerged as the de facto industry standard for applied machine-learning and artificial intelligence because of its readable, flexible syntax, extensive library and package support, and excellent interoperability. Pandas and NumPy are standard data manipulation and numerical computation libraries that form the core of every data-driven Python project (McKinney, 2010; van der Walt et al., 2011). Scikit-learn is a library that has established itself as the standard interface to supervised learning for the majority of modern machine-learning practitioners, with unified APIs for model implementation, tuning, evaluation and pipelines (Pedregosa et al., 2011) \cite{pedregosa2011}. In the context of the present study, SHAP and LIME provide the implementation backbone of the chosen explainability approach.

The theoretical background presented in this chapter demonstrates the fact that student academic performance prediction rests at the intersection of diverse but coherent streams of research in educational data mining, machine learning and eXplainable AI. This connection between fields provides both conceptual and technological scaffolding for the present work and shows that effective educational data analysis requires not only high-performing predictive models, but also transparent interpretability mechanisms that can translate model outputs into pedagogically significant knowledge. The structure of Chapter 1 is guided by this logic and the order of sections follows the natural logic of moving from broader domain context to narrower methodological focus. Section 1.1 is dedicated to setting up the core concepts and domain context of the student performance prediction problem in higher education. Section 1.2 provides an overview of educational data mining and the history of student performance modelling. Section 1.3 reviews supervised machine-learning methods that can be used in the context of academic outcome prediction. Section 1.4 discusses explainable artificial intelligence approaches and their educational applications. Section 1.5 addresses evaluation metrics and interpretability criteria for student analytics. Finally, Section 1.6 offers a summary of the main theoretical results and discusses their implications for the empirical work in the rest of the thesis. Taken together, the theoretical synthesis outlined in this chapter provides the scientific and methodological foundation for the implementation phase of this thesis. Anchoring the later, empirical analyses in established scientific theory is an important step towards adhering to international research quality standards and making a contribution to the field of educational data analytics.

\section{ACADEMIC PERFORMANCE IN HIGHER EDUCATION AND LEARNING ANALYTICS}
\label{sec:concepts}

Academic performance may be broadly defined as the extent to which students meet the intended learning outcomes as established by the curriculum or educational institution. In the context of higher education, academic performance is operationalised at various levels, such as courses, degree programs, and overall institutions, through different metrics, such as course grades, cumulative GPA, number of credits earned, course completion, and graduation. These metrics serve multiple purposes, such as assessing individual student learning and progress, evaluating institutional quality and outcomes, informing program improvement, and shaping quality assurance processes. With the datafication of learning and education in higher education institutions, academic performance has become a common target for various learning analytics or educational data mining methods. Learning analytics is defined as “the measurement, collection, analysis and reporting of data about learners and their contexts for purposes of understanding and optimizing learning and the environments in which it occurs” (Ferguson \& Clow, 2017) \cite{ferguson2017}. This is different from traditional educational research as this paradigm is underpinned by the idea of integrating learning theory, data science and information systems in order to derive meaningful insights and make evidence-based inferences from large educational datasets. Learning analytics leverages the capability of collecting and storing massive amounts of digital data in institutions in order to monitor and assess student learning and development on a continuous and real-time or near–real-time basis, which makes it possible to predict student performance and progress using a wide range of data sources and machine-learning models (Ferguson \& Clow, 2017) \cite{ferguson2017}.

Academic performance is a multi-faceted construct affected by a number of cognitive, behavioural and contextual factors. Cognitive factors include prior academic performance, domain-specific knowledge, and learning strategies. Behavioural factors include attendance, participation, engagement with learning materials, and submission of assessments. Contextual factors include socio-economic status, institutional and program factors, course design, and instructional quality, among other factors. It is also important to note that the effect of any of these variables on student performance can vary across different contexts. It is worth noting that a body of research has shown that no single factor can sufficiently explain student performance, instead, student performance is the product of the complex interplay of several mutually dependent and interrelated variables (Bhanpuri et al., 2015) \cite{bhanpuri2015}. From the data-analytic perspective, student academic performance datasets are typically represented as a table of instances. Instances can represent students themselves or a student–course combination, or snapshots of a student’s academic trajectory in the form of timestamps or semesters (Saqr et al., 2017) \cite{saqr2017}. In the first case, the student dataset may contain static attributes describing their background and demographics. In the second case, the table may contain dynamic attributes, such as grades collected over time or data on the interactions with learning materials captured by the institution’s learning management system. This data can be heterogeneous in its nature, meaning that datasets can have mixed-type attributes (categorical and numerical), can have missing values, may have different class distributions, and the relative importance of different attributes can vary across domains and datasets (Saqr et al., 2017) \cite{saqr2017}. The use of digital learning platforms in the higher education context has expanded the range of student data collected over time. With the use of learning management systems, online quizzes and exams, and student information systems, institutions have been collecting increasingly large volumes of fine-grained data on student learning activities on an ongoing basis. In addition to descriptive analytics, such data allows for leveraging predictive and prescriptive analytics in order to inform the design of early-warning systems to prevent student failure and dropout early before it is too late (Paneva-Marinova, 2006) \cite{paneva2006}.

There are some important theoretical and methodological considerations that must be kept in mind when applying academic performance prediction. The first is the difference between correlation and causation. Machine-learning models can be used to identify patterns and associations between various features and the student academic performance but do not directly point to causes and effects. This means that while a model may associate a low level of engagement with low grades, the former may not be a direct cause of the latter. For example, a low number of access to the learning platform can be as a result of a student experiencing health issues, personal problems, or financial hardships. This distinction between correlation and causation is particularly important for prescriptive analytics, as interventions based on the model predictions are expected to be evidence-based, and must have clear pedagogical justification and be educationally sound (Gašević et al., 2014) \cite{gasevic2014}. The second consideration is that of the data quality and representativeness. The datasets collected from educational contexts typically have missing values and may be subject to biases that can be intrinsic to the institution’s policy and student processes or artefacts of the data-collection process (Kuzilek et al., 2017) \cite{kuzilek2017}. For example, attendance records can be missing, students can self-report their demographic information or students may be subject to different assessment regimes in different courses or at different institutions. This can impact both accuracy and fairness of machine-learning models if not taken into account during data-preparation and model evaluation steps. A number of recent studies on learning analytics and educational data mining methods have emphasised the importance of rigorous data preprocessing and validation steps in order to ensure the reliability of the modelling outcomes (Kuzilek et al., 2017) \cite{kuzilek2017}. The growing use of predictive models in education has raised ethical questions regarding issues of transparency and explainability, accountability, and student agency. Predictions that are used in decision-making, such as academic advising, scholarship allocation, or progression, can be subject to misconceptions, misinterpretations, and errors, particularly in cases where the model can be an opaque black box that is not understandable to students and educators alike. A growing body of work in learning analytics and educational data mining research has emphasised the need to provide accurate and fair predictions together with interpretable explanations of model predictions (Tsai et al., 2019) \cite{tsai2019}.

A more theoretical consideration regarding academic performance prediction relates to its formulation as a machine-learning problem. In some cases, this can be seen as a classification or regression problem depending on how the outcome variable is defined. Binary or multi-class classification can be used to predict binary or categorical outcomes, such as pass/fail, dropout/retention, or academic risk level. In contrast, regression models are used to predict continuous outcomes, such as the final exam scores or the final GPA. The formulation of the prediction task and the related evaluation metrics have implications for model selection and training and for the interpretability of the resulting models, and should be aligned with the learning analytics use case (Lakkaraju et al., 2015) \cite{lakkaraju2015}. Recent work in learning analytics and educational data mining research has also placed increasing emphasis on the need to align the design of the predictive modelling approaches with pedagogical goals, meaning that the design of the predictive model should take into account the purpose it is expected to serve, and that these should not be designed to maximise predictive accuracy alone, but also be actionable and pedagogically meaningful. In other words, academic performance prediction should be seen not as an end in itself, but as a means to an end (Wise \& Jung, 2019) \cite{wise2019}. This perspective requires explainable predictive models that can provide insights into key performance drivers that can be used to identify students that may require academic support, such as support programs or curriculum changes. As a result, academic performance prediction is increasingly framed not as a standalone technical exercise, but as a socio-technical system situated in the educational context. To sum up, in this section, I have provided an overview of the concept of academic performance as it is used in higher education context, the uses and applications of learning analytics and educational data mining methods for its analysis and prediction. This has been done in order to help contextualise the methods and techniques discussed in the rest of this chapter, and to highlight the theoretical and methodological issues that should be taken into account when considering approaches for academic performance prediction.


\section{EDUCATIONAL DATA MINING AND STUDENT PERFORMANCE MODELLING}
\label{sec:related}

Educational data mining (EDM) is an interdisciplinary field that studies how to apply data mining and machine learning techniques to educational data. It aims to develop and use computational methods to analyse and understand the patterns, processes, and outcomes of learning and education. EDM differs from educational statistics in that it is more focused on prediction, discovery, and automation of knowledge extraction from large and complex data sets, rather than on testing hypotheses, generalising from samples, and evaluating interventions at the aggregate level. Student performance modelling is a well-established topic within the scope of EDM. A performance model is a computational representation that captures the relationship between observable student characteristics and their academic outcomes. Performance models are typically built using historical student records that contain various attributes such as prior grades, engagement metrics, demographic information, and institutional factors. The goal is to learn a decision boundary or a functional mapping that can be used to predict future academic outcomes given new or unseen observations. Performance models are useful for early-warning systems, adaptive learning systems, and institutional dashboards. The mathematical foundation for student performance modelling comes from supervised learning, where a labelled dataset is used to train a predictive model. Labels are binary or categorical indicators of academic outcomes, such as whether a student passed or failed a course, dropped out or not, achieved a certain grade or level, or reached a threshold of GPA. Input features are the observable characteristics that are used to make the predictions. They can include prior grades, attendance, engagement, online learning system logs, assignment submission patterns, demographic variables, and other relevant information. The choice of features is often based on both domain knowledge and data-driven relevance, as not all attributes in the dataset may be predictive or interpretable. EDM research has shown that student performance prediction is a context-dependent problem. Models that are trained and validated on data from one institution, program, or cohort may not generalise well to other contexts without retraining or adaptation. This is because of the variability in curriculum design, assessment methods, grading schemes, and student populations across different educational settings. For this reason, recent literature emphasises the importance of using recent and context-specific datasets for model development rather than using benchmark datasets collected in the past under different conditions (Tomasevic et al., 2020) \cite{tomasevic2020}.

In the early days of EDM, simple classifiers such as Naïve Bayes, decision trees, and k-nearest neighbours were used for student performance prediction. These methods are easy to understand, implement, and scale with large datasets. However, as the size and complexity of educational data grew, more sophisticated algorithms were developed and adopted. Ensemble methods such as random forests and gradient boosting have become popular in EDM due to their high accuracy and robustness to noisy or imbalanced data. These methods can also capture nonlinear relationships and interactions between features that are hard to model with linear methods. One of the challenges in student performance modelling is the issue of class imbalance. In many educational datasets, the number of students who fail or drop out is much smaller than the number of students who pass or graduate. This imbalance can lead to biased or inaccurate predictions if the classifier is not properly trained or evaluated. To address this issue, EDM researchers have proposed various methods to resample, reweight, or penalise the data to balance the classes. The choice of evaluation metric is also crucial for assessing the performance of student performance models. Accuracy alone is not a reliable measure when the classes are imbalanced or when the cost of different types of errors is not the same. EDM researchers have suggested using more informative metrics such as recall, precision, F1-score, or area under the ROC curve to evaluate the predictive performance of the models on both the majority and the minority classes (Tempelaar et al., 2020) \cite{tempelaar2020}. Another important aspect of EDM is feature engineering. Raw educational data often needs to be transformed or manipulated to create features that are more suitable for modelling. For example, temporal aggregation of engagement events, normalisation of scores, encoding of categorical variables, and imputation of missing values are common preprocessing steps. Feature engineering not only improves the predictive power of the models, but also affects their interpretability. Derived features need to be meaningful and understandable in the educational context. Poorly designed or irrelevant features can lead to spurious correlations or misleading explanations.

More recent EDM literature has also raised concerns about the lack of interpretability of purely predictive approaches that do not offer any understanding or explanation of the factors that influence academic outcomes. Models that have high accuracy but are not transparent or accountable to the educational stakeholders and decision-makers are of limited value. This has led to a paradigm shift from accuracy-first evaluation to a more balanced and holistic framework that considers both the predictive performance and the explanatory usefulness of the models. As a result, student performance modelling is increasingly complemented by explainability methods that allow the inspection and justification of model outputs. The recent advancements in explainable machine learning have influenced the development and application of EDM methods as well. Instead of simply selecting the model with the highest accuracy, EDM researchers and practitioners are now evaluating models based on their interpretability and alignment with pedagogical reasoning as well. Linear models such as logistic regression are still widely used as baselines because of their coefficient interpretability, but tree-based ensembles are often used in combination with post-hoc explanation methods to achieve both performance and understanding. This hybrid approach enables the use of complex models without compromising accountability. Ethical and fairness issues are also an important part of EDM research. Student performance models may encode or amplify biases that exist in the historical data or the educational system, leading to unfair or inaccurate predictions for certain groups of students. Recent research has highlighted the need for bias detection, fairness-aware modelling, and explanation of predictions to ensure the ethical and responsible use of EDM. Regulatory and ethical guidelines are also increasingly requiring that automated systems that support educational decisions be auditable and interpretable, which further emphasises the importance of explainable EDM approaches.

From a systems perspective, EDM-based performance models are typically deployed as components or modules within an institutional information system or an analytics platform. These systems integrate data ingestion, data preprocessing, model inference, and result visualisation into a cohesive workflow. The system needs to be reproducible, scalable, and maintainable, which is facilitated by modern data-science technologies and ecosystems. Scripting and programming languages such as Python allow for rapid prototyping and experimentation with various methods and models, while supporting production-ready analytics through modular design, version-controlled workflows, and deployment automation. In conclusion, EDM provides the methodological and conceptual background for student performance modelling. The field spans supervised learning, feature engineering, evaluation, and ethical considerations for mining and understanding educational data. Recent developments in EDM reflect the increasing emphasis on recent and context-specific data, robust evaluation, and model interpretability. These trends set the scene for the machine-learning and explainability techniques that are presented and used in the following chapters.


\section{ SUPERVISED MACHINE LEARNING METHODS FOR ACADEMIC OUTCOME PREDICTION}
\label{sec:theory}

Supervised machine learning is the dominant approach used in the contemporary educational data mining literature to predict academic performance. Supervised learning algorithms build a predictive model from a labelled dataset where each example is tagged with an outcome variable. The task then becomes to learn a decision function that can predict an outcome for unseen examples. In the case of learning analytics, the outcome variable of interest would be some representation of academic performance, such as whether a student failed or passed a course, is at risk of academic probation or attrition, achieved a low or high grade, or passed with an overall average mark above a threshold value. Prediction tasks are typically framed in terms of either classification or regression problems, depending on the nature of the outcome variable. Classification is used when the outcome is discrete, for example, pass/fail, at-risk/not at risk, or low/medium/high. Regression is used when the outcome variable is continuous, for example, a final numeric grade or overall GPA score. The majority of learning analytics applications that focus on prediction of academic performance and outcomes use classification problems, as this better aligns with institutional processes that need to intervene at a binary level. As such, the thesis also takes a classification-orientated view. A range of different supervised learning approaches are reviewed and used for academic performance prediction, with their advantages and disadvantages considered in comparison.

Linear classifiers are some of the earliest methods of supervised learning and are still commonly used for supervised learning problems. Logistic regression, a form of linear regression adapted for the classification problem, is the most well known and commonly used of these methods (James et al., 2021) \cite{james2021}. Logistic regression is a parametric model of the relationship between the input features and a binary outcome, expressed as a conditional probability. The model is linear in the log-odds of the conditional probability and parameters of the model can be learned using maximum likelihood. Regression coefficients can then be used to interpret the direction of feature effects on the outcome. Regularised forms of the linear regression classifier, such as L1 and L2 regularisation, are commonly used to counteract multicollinearity in the feature set (James et al., 2021) \cite{james2021}. Linear models, however, have limitations. One of the principal limitations of using a linear regression model for predicting academic performance is that the relationships between many of the input features are likely to be non-linear. For example, a student’s performance on a course is likely to be affected by an interaction of their prior performance, engagement with the course, nature of the assessments, and the institution that they are studying at. The relationship between such features and the target variable are unlikely to be linear and the decision function that is learned using a linear model is only an approximation of the true relationship. To model more complex decision functions, non-linear supervised learning techniques have been developed. One of the most well known and common non-linear techniques are decision tree–based methods. Decision trees (Cortez \& Silva, 2008) \cite{cortez2008} recursively subdivide the input feature space into regions based on splitting rules for each feature. At each node in the tree a splitting rule is used that partitions the input space in a way that the resulting child nodes contain more homogenous (pure) distributions of outcomes. Decision trees are often used for classification and regression tasks as the decision function that they represent is both interpretable and capable of approximating non-linear functions. However, they are prone to overfitting and can be unstable, with small changes to the training data causing large changes to the decision function (Hastie et al., 2017) \cite{hastie2017}. Ensemble methods, such as Random Forest (Akçapınar et al., 2019) \cite{akcapinar2019}, that combine the predictions of many decision trees have been shown to result in more accurate and stable predictive models. Random Forest uses a combination of bootstrap sampling to draw training data to learn each individual tree and random subsetting of the features to select the splitting rules at each node. The prediction of the Random Forest is then generated by aggregating the predictions of the individual trees through voting (classification) or probability averaging (regression). Predictions from Random Forest models have been shown to have high predictive power for dropout and failure risk in educational settings across a wide range of contexts and applications.

Gradient Boosting (Saxena \& Garg, 2017) \cite{lundberg2017} is another family of ensemble methods based on the aggregation of decision trees. The main difference between gradient boosted and random forests is in the way that individual trees are combined. Rather than each tree being trained independently of the others, gradient boosting approaches learn a sequence of trees such that each tree is trained to correct the errors of the previous one. This results in an additive model of decision trees that is much more powerful than each individual tree and can result in superior prediction performance. The gradient boosting algorithm iteratively fits a decision tree to the current residual errors in the predictions from the existing ensemble and then adds it to the model. Implementations of gradient boosted trees, such as Gradient Boosting Machines and Extreme Gradient Boosting (XGBoost), are commonly used for structured-data prediction tasks and have been shown to achieve state-of-the-art performance in many tasks. In educational settings they have also been shown to result in accurate predictions of dropout risk and other binary outcome measures. The methods are well-suited to high-dimensional feature sets that are common in learning analytics, such as student datasets, and have been shown to be able to capture complex interactions in these feature sets (Chen \& Guestrin, 2016) \cite{chen2016}. Neural networks are also able to approximate non-linear functions and have been used for educational performance predictions as well. Feedforward neural networks are an example of a neural network–based method that can be used to learn non-linear decision functions. Recurrent neural networks and long short-term memory architectures can also be used to capture the sequential nature of time-ordered events (Domingos, 2012) \cite{hernandez2012}. This type of method is often used for modelling clickstream data and temporal engagement patterns extracted from learning management systems.

The selection of an appropriate supervised learning approach for the task of academic performance prediction also needs to consider several factors. The size of the dataset, its feature dimensionality, and constraints on data quality can all have a bearing on the choice of algorithm. Ensemble methods, such as Random Forest, have been shown to be effective even with many weakly performing base learners. As a result, they are robust to many of these common data problems. Imbalanced classes, whereby students who are at-risk of an outcome, such as dropout or failure, form a minority class, is another common problem with datasets in educational settings. This often occurs as these outcomes form the minority of the population of students. Machine learning algorithms also need to be evaluated using appropriate metrics to ensure that their performance is not overestimated. Standard metrics, such as precision and recall, F1-scores, or area under the receiver-operator curve, are often used (Tempelaar et al., 2020) \cite{tempelaar2020}. Feature selection and feature representation can also play an important role in the supervised learning approach taken for a given task. Feature selection is often used as a pre-processing step to improve the interpretability of a model and prevent overfitting (James et al., 2021) \cite{james2021}. High dimensional feature spaces often have redundant or sparsely contributing features that can reduce the ability of a model to generalise to unseen data. Embedded feature selection is an alternative to this, whereby feature selection is done internally as part of the training process. Embedded methods are often available as part of the implementation of a model, for example, in the form of regularisation or feature importance measures. In the case of learning analytics feature importance, can be especially useful, but care must be taken when interpreting the results of statistical methods with regards to their pedagogical meaning. Supervised learning approaches also typically require validation of the learned models to provide an unbiased estimate of their performance in real-world contexts. Cross-validation is a common method of model validation where the data is split into training and validation subsets, where the model is trained on the training data and its performance is assessed on the validation set. Stratified sampling is often used to ensure that the class distribution of the validation set is approximately the same as the original dataset. Temporal validation is used if the dataset contains a time-ordered structure. This involves ordering the data and splitting the data at a time point, so that the validation data is always more recent than the training data. This prevents information leakage and ensures that model performance is evaluated on a realistic basis.

As supervised learning approaches are increasingly used for real-world decision-making in education, issues of fairness and equity are increasingly coming to the fore. Supervised models will reflect and amplify inequalities in the training data on which they are trained. These can result in differences in predictions across demographic groups that are associated with sensitive characteristics such as race or gender. Supervised learning algorithms do not address these biases themselves and careful evaluation of models and fair and transparent reporting of their use and behaviour is therefore needed (Holstein et al., 2022) \cite{holstein2022}. In the implementation of supervised learning for academic outcome prediction, a number of machine learning best practices are also followed. A standardised and modular data pipeline is used that encompasses pre-processing, model training and evaluation, and inference. Python-based ML libraries also provide standard interfaces to ML methods to allow for easy reproducibility and comparison. The use of the same data pipeline for all supervised learning methods used for the comparative study allows for the models to be trained and tested under the same conditions. The supervised learning methods used for the comparative study are chosen to balance the need for prediction performance, explainability, and other factors. Logistic Regression is used as a baseline method due to its interpretability. Random Forest is used as it is a robust, non-linear method, and Gradient Boosting as it has high predictive power on structured datasets. The comparative evaluation of these methods also lays the groundwork for later comparison of explainability techniques.

\section{EXPLAINABLE ARTIFICIAL INTELLIGENCE (XAI) IN EDUCATIONAL APPLICATIONS}
\label{sec:listings}

The popularity of machine learning in educational performance modelling has brought increasing attention to the explainability and interpretability of the employed methods. Predictive models in education are often applied to high-stake scenarios such as early warning, academic advising, and resource allocation, where they may directly affect students’ trajectories. Unexplainable black-box systems can produce predictions that are challenging to trust, validate, or justify on ethical grounds. As a result, the limits of predictive accuracy as the sole evaluation metric have gained more recognition in the field, leading to a call for transparency and interpretability mechanisms for predictive models Explainable Artificial Intelligence (XAI) is a set of approaches and principles to support understanding, interpretation, and critical evaluation of machine learning outputs. Explainability in educational performance prediction is important for both technical validity and pedagogical or institutional accountability. It can be used to validate research methods by confirming that a model bases its predictions on plausible academic rather than non-academic or spurious features, including outliers or noise. Explainability can also serve as a bridge to educational practitioners, providing human-interpretable information on prediction causes for use cases such as academic advising. In the context of this thesis, XAI is applied at two levels. Global model-level explanations of feature importance are used to validate that the most impactful variables on performance are pedagogically meaningful and do not represent proxies or spurious relationships. Local explanations are used to provide transparent and comprehensible performance predictions for individual students.

Approaches to machine learning explainability and interpretability can be classified according to several dimensions and schemes (Table 1.1). One way to categorise methods is based on their level of model dependency and timing of interpretation. Inherently interpretable models are often based on simple mathematical or algorithmic structures that are human-readable. Some methods require access to the model itself and train the explanation model jointly with the main learner, while other methods are model-agnostic and can be applied after training a predictive model of any type. This work focuses on the post-hoc, model-agnostic approaches, which include a range of both local and global methods for student performance modelling (Table 1.1). A second classification dimension distinguishes between global and local explanations, based on their level of granularity. Global explanations summarise and interpret an entire predictive model across the whole dataset, while local explanations provide detailed interpretability of single predictions at the student or other unit level. The main families of explainability approaches for machine learning models are summarised in Table 1.1. This provides the background and general terminology for the thesis.

\begin{table}[h]
\caption{Categories of Explainable AI Methods}
\label{tab:1.1}
\centering
\begin{tabularx}{\textwidth}{@{} l >{\raggedright\arraybackslash} p{3.5cm} >{\raggedright\arraybackslash} p{2.5cm} >{\raggedright\arraybackslash} X @{}}
\toprule
\textbf{XAI Category} & \textbf{Description} & \textbf{Analytical Focus} & \textbf{Educational Relevance} \\
\midrule
Intrinsically interpretable & Directly understandable structure & Coefficient inspection & Transparent baseline models \\
Post-hoc model-agnostic & Post-training explanations & Black-box prediction analysis & Interpret student risk models \\
Global explanations & Overall model behavior & Feature importance & Key academic success drivers \\
Local explanations & Individual predictions & Case-level analysis & Personalized student feedback \\
Surrogate models & Simplified model approximations & Simplified interpretation & Non-technical communication \\
\bottomrule
\end{tabularx}
\smallskip
\small Source: Author’s compilation based on Arrieta et al. (2020) and Molnar (2022).
\end{table}


The taxonomy in Table 1.1 shows that different approaches to explainability may be necessary to address different types of analytical goals. Both global and local levels of granularity are necessary for student performance prediction tasks. The combination of global and local explainability methods is also behind the observed trend towards post-hoc, model-agnostic approaches as the dominant explainability paradigm in machine learning. These methods are often preferred over inherently interpretable models for performance prediction tasks, since they allow using state-of-the-art high-performing models without being restricted to models with more transparent structures. Linear regression and decision trees still have practical application in education due to their inherent interpretability. However, they are known to struggle with datasets that contain interactions or non-linear relationships, which are common in student performance data. Therefore, post-hoc explainability methods represent a method to provide interpretability on top of such models. This is also particularly important for operational systems in education, which need to balance analytical requirements of accuracy and interpretability with other constraints and criteria, including ethical requirements.

The two most widely adopted post-hoc explainability methods in applied machine learning, including educational settings, are SHAP and LIME. Both aim to provide intuitive and human-interpretable explanations for individual predictions of complex, black-box machine learning models by attributing an importance weight to each input feature. The two approaches differ in their theoretical grounding and in their practical behaviour. The popularity of both SHAP and LIME in real-world applied machine learning has been growing, as they both support different use cases and balance interpretability and fidelity to the original predictive model in different ways. They also have complementary methodological properties that can be advantageous for educational performance prediction (Table 1.2). In this study, the two methods are used in a similar way, and their methodological differences do not strongly affect their integration with performance prediction tasks. Their adoption as the primary XAI methods in this work was motivated by their strong practical track records and active communities of developers and adopters. Their properties that make them especially suitable for this thesis include their support for both local and global explanations, as well as human-readable output and transparent underlying mathematics.

\begin{table}[htbp]
\centering
\caption{Comparison of SHAP and LIME for Educational Performance Prediction}
\label{tab:shap_lime_comparison}
\renewcommand{\arraystretch}{1.3}

\begin{tabular}{|
>{\RaggedRight\arraybackslash}p{5.5cm} |
>{\RaggedRight\arraybackslash}p{4.5cm} |
>{\RaggedRight\arraybackslash}p{4.5cm} |}
\hline
\textbf{Criterion} & \textbf{SHAP} & \textbf{LIME} \\
\hline
Theoretical basis &
Game theory (Shapley values) &
Local surrogate modelling \\
\hline
Explanation scope &
Global and local &
Local only \\
\hline
Consistency guarantees &
Yes &
No \\
\hline
Computational cost &
Moderate to high &
Low to moderate \\
\hline
Stability across runs &
High &
Sensitive to sampling \\
\hline
Suitability for cohort analysis &
High &
Limited \\
\hline
Suitability for individual advising &
High &
High \\
\hline
\end{tabular}
\vspace{0.3em}
\small Source: Author’s compilation based on (Lundberg et al. (2020)) and (Ribeiro et al. (2016)).
\end{table}

The comparison of SHAP and LIME in Table 1.2 shows that the methods are, in many respects, complementary to each other, rather than directly competing for similar use cases. SHAP is used in this study for global and local analysis of student performance data, to gain insights into performance drivers at both the cohort and the individual student levels. LIME, on the other hand, is used to provide individual student-level predictions in a format that can be intuitively understood and used by educational practitioners such as student advisors and teachers. This can be helpful in a wide range of use cases including one-on-one academic advising sessions, student-level prediction diagnostics, and making predictions and recommendations with student and parent involvement. In summary, both SHAP and LIME are used to ensure that the analytical framework and its educational use cases have both the necessary macro- and micro-level explainability. Explainability also plays an important role in validating student performance models and for bias and fairness analysis, including analysis of proxy variables and sensitivity to student and institutional characteristics. The feature attributions produced by XAI methods such as SHAP and LIME can be used to diagnose the factors driving the model prediction and to determine if models are overly reliant on sensitive or potentially non-causal features. Global SHAP summary plots as well as local explanations can help to validate that models give relatively more weight to pedagogically relevant student and institutional characteristics, such as prior achievement and engagement, rather than proxies of academic success such as socio-economic status and other variables. In addition to method validation and bias analysis, XAI can also be seen as important for stakeholder communication and knowledge transfer. Transparency requirements in applied educational settings and decision-makers’ lack of technical expertise in machine learning mean that explanations are required to bridge the gap between high-level data patterns and educational decision-making. The transparent communication of information on academic performance drivers is a critical step for real-world adoption and impact of the prediction systems in educational settings. For example, explainable educational systems are more likely to be used and supported in practice than less transparent models, even when the predictive accuracy of the latter is only marginally higher.

From the methodological perspective, the inclusion of explainability also has implications for the relation between prediction tasks and model evaluation. Evaluation metrics should be chosen to be aligned with the downstream use case of explainability, rather than being solely accuracy-based. Predictive accuracy should not be seen as an end in itself in human-centred educational learning analytics, but instead as a necessary prerequisite for application in real educational settings. This emphasis on human-centred evaluation of machine learning models is consistent with a broader paradigm shift towards human-centred learning analytics. XAI in educational analytics can be seen as a link between supervised learning and human-centred evaluation of academic performance models, building on their prediction performance but supporting transparency, actionability, and validation. In this study, SHAP and LIME explanations of performance prediction models will be used to support both global model understanding and local prediction transparency (Figure 1.1). These two tasks will be used to identify and validate the most important academic performance drivers and to generate human-interpretable performance predictions and recommendations that can be included in decision-support systems for students and other stakeholders.

The general comparison of the explainability approaches introduced in the previous subchapter has shown that the choice of XAI algorithm is not only a technical but a methodological decision that is steered by the goals of the analysis. Explainability in the context of predicting student academic performance is needed on multiple levels of abstraction, as the prediction task itself covers both the global and local scales. On the one hand, XAI should produce explanations on a macro-level to explain the general drivers of success and failure, and on the other hand, local explanations are needed to justify individual predictions. The interpretability approach chosen for this analysis should thus be able to bridge the macro- and micro-levels. The global explainability introduced with SHAP allows one to measure feature importance on the aggregate level, thus identifying factors which are the most predictive for the student performance. Features in this regard often include prior academic achievement measures, indicators for attendance, performance in different types of assessments, and other engagement-related measures. When considered on the global scale, these explanations can reveal information about structural patterns in the learning data, allowing the institution to understand which variables are the most significant predictors of student success and should be considered when tuning their approach. The global explanations are particularly useful for academic administrators, curriculum designers and policy-makers, as they enable data-driven insights into the structure of educational interventions to support evidence-based changes to teaching practices, evaluation policies, and student support programmes.

The local explanations, on the other hand, demonstrated with LIME-based approach, are useful in individual cases. As prediction is usually followed by a human decision or some type of interaction in the educational context, local explanations allow one to interpret predictions on a case-level. For instance, after making a prediction, the human agents are able to use a local explanation to understand why a particular student is assigned to a particular risk or success group, which in turn supports a more informed and collaborative discussion between a teacher and a student, and minimises the risk of over-reliance on a raw prediction value. The explainability is also important in terms of making sure that a predictive model is functioning properly and, in fact, learned meaningful academic relations and was not instead just ‘cracked the code’ on the training data. The feature attributions can be examined to see which variables the model was most heavily relying on, which provides information about the validity of the model itself. For instance, if features such as attendance or engagement with non-course materials were the most important in most cases, it would indicate that the model is looking for action-based and more directly changeable relationships rather than demographic or other potentially biased proxies. The insights produced by the explainability analysis are thus useful in terms of the methodological quality of this work, which is important for supporting transparency and validity of education-related predictive analytics. Another aspect of explainability is fairness and ethical considerations, as the use of any educational prediction system immediately has a more significant impact on students than for predictive systems used in lower stakes domains. The explainability techniques provided by explainable ML support fairness in terms of identifying possible discrepancies in global explanations across different demographic subgroups, by comparing the feature contributions. The differences in the SHAP summary plots for different student cohorts can be an indicator that the model is behaving differently for specific groups, thus allowing for responsible use of AI and identification of possible bias in a machine learning system. Overall, the introduction of the explainability paradigm in this work is guided by these multiple use cases, in which feature attributions can support the interpretation and validation of the results, guide fairness assessment and responsible AI usage and, by extension, help make the predictions useful in real-world educational settings.

From the practical systems design perspective, explainability also influences the integration of prediction and decision-support workflows into existing work processes. Machine learning models that provide interpretable and meaningful explanations are more likely to be adopted by an educational institution because they can be integrated into the existing structures of decision-making that are based on human cognition. Instructors, counsellors, and other professional groups will not trust a prediction unless they can be explained in an academically meaningful way. Thus, explainable predictions that are summarised in an easily understandable visual form through feature importance plots, feature weights for a case, or other forms of explainability data visualisation are more usable for stakeholders, thus reducing the risk of model rejection. The presentation of SHAP and LIME earlier also already introduced the stability issue with some types of explanations. The SHAP-based global explanations are much more stable and consistent across runs than the LIME-based ones, which has some implications for the analytical decisions regarding which explanations to use for which tasks. This is also the reason for not using a single XAI algorithm but instead choosing both of them as supplementary approaches. Explainability also has an additional use case when it comes to the interpretation of the evaluation results. Evaluation measures in the next chapter are only a quantitative and partial view on the model’s performance and, on their own, do not reveal what features are driving good or bad predictions. Feature attributions allow one to see which variables drive which predictions, and therefore can be used to support the quantitative measures with a richer set of insights into the model’s predictions. This holistic evaluation strategy is an important component of this work’s methodology. From the perspective of educational research, interpretability also helps with the theoretical grounding of a study. Transparent and explainable models, and in this case specifically the global explanations, allow one to see if the results correspond to the existing theories on academic success, which can be a supportive evidence for either. If, on the other hand, the predictions contradict the existing theoretical approach, it might signal that the theory should be updated or additional contextual information should be collected to take into account local specificities. In this work, explainable artificial intelligence is integrated into the pipeline for predicting students’ performance in order to meet the multiple methodological requirements that are described above. It supports validation of the model, fairness assessment, stakeholder communication, and theoretical grounding of the results. All of these functions allow this work to provide scientifically rigorous and practically valuable educational data science. For this research, explainability is the mechanism of transforming predictive results into educational knowledge and the analytical framework introduced does not treat explainability as a post-processing element of the modelling pipeline but as one of its core components that is intertwined with other elements. The combination of global and local XAI techniques allow this work to ensure that the accuracy of the predictions is accompanied by the clarity of their interpretation. This conceptual coupling of explainability and prediction is the guiding idea for the next methodological chapters where the specific XAI techniques are operationalised in the context of the machine learning models. The focus on interpretable output will support the critical assessment, ethical justification, and real-world practicality of the empirical findings.

\section{EVALUATION METRICS AND MODEL INTERPRETABILITY IN STUDENT ANALYTICS}
\label{sec:motivation}

The process of training machine learning models for student academic performance prediction naturally leads to a question of how the model’s predictions should be evaluated. A comprehensive evaluation strategy is required to ensure that a given model produces not only stable and reliable but also educationally meaningful results. The evaluation metrics in learning analytics have dual purposes – besides providing technical characteristics of predictive models they also serve as proxies for justifying or refuting the use of such models in decision-support environments. This is particularly relevant for the problem of academic performance prediction, where class imbalance, multi-class outcomes, and interpretability requirements make the choice of evaluation metrics directly influence the perceived quality of research. For example, educational data typically have an unequal distribution of failed and passed students, rendering simplistic accuracy metrics uninformative. A prediction model that naively labels all data instances with the majority class can thus be reported as “accurate” without performing any risk detection. To address this issue, the model performance in the educational data mining literature is evaluated from multiple perspectives using a combination of complementary metrics that characterise the quality of classification. They allow for an assessment of both the predictive correctness and the practical impact of potential misclassification.

In addition to correctness of predictions, it is also important to assess whether the model results are generalisable to unseen data or if they are overfitted to the training data. The problem of overfitting is particularly relevant for student performance prediction due to the sample size limitations and correlations between some of the features used. Robust performance evaluation, therefore, requires time-aware data splitting, cross-validation, and stability analysis. The evaluation framework needs to be consistent with the natural temporal ordering of academic data. In particular, the method of constructing the training and testing sets needs to preserve the causal ordering between the features used to make predictions and the outcome variable. The most common evaluation metrics for binary and multi-class classification of academic performance and the brief interpretation of those metrics are summarised in Table 1.3. These metrics form the basis for performance evaluation of the different machine learning models used in this thesis.

\begin{table}[h]
\caption{Evaluation Metrics for Student Academic Performance Prediction}
\label{tab:1.3}
\centering
\begin{tabularx}{\textwidth}{@{} l >{\raggedright\arraybackslash} p{3cm} >{\raggedright\arraybackslash} p{2.5cm} >{\raggedright\arraybackslash} X @{}}
\toprule
\textbf{Metric} & \textbf{Definition} & \textbf{Analytical Focus} & \textbf{Educational Interpretation} \\
\midrule
Accuracy & Ratio of correct predictions to total & Overall correctness & Limited usefulness with imbalanced classes \\
Precision & Predicted at-risk truly at risk & False positive control & Avoids unnecessary interventions \\
Recall (Sensitivity) & Actual at-risk correctly identified & False negative control & Critical for early-warning systems \\
F1-score & Harmonic mean of precision \& recall & Balanced performance & Suitable for imbalanced datasets \\
ROC-AUC & Area under ROC curve & Ranking quality & Measures discrimination capability \\
\bottomrule
\end{tabularx}
\smallskip
\small Source: Author’s compilation based on Sokolova and Lapalme (2009) and Baker and Inventado (2016).
\end{table}

The selected evaluation metrics in Table 1.3 are intended to capture the multi-dimensional nature of performance assessment in educational analytics. Precision and recall are particularly useful in the academic setting as they describe two types of risk that institutions may be exposed to. A high recall rate implies that a larger proportion of students who require some form of support or intervention are being identified by the prediction system. At the same time, a high precision rate ensures that the total number of identified students is kept at a minimum to avoid putting unnecessary strain on institutional resources and discouraging the students’ morale. The F1-score can be used as a summary metric when the optimisation of one of the two aforementioned metrics is not desirable. ROC-AUC provides a complementary view by focusing on the model’s ability to rank students by their risk level as opposed to classifying them based on a fixed decision threshold. It is especially useful when the institution wants to prioritise the allocation of support to those at the highest risk rather than simply flagging everyone as either “safe” or “at risk”. The multi-metric evaluation approach therefore ensures that the model assessment aligns with the educational objectives rather than technical optimisation.

Evaluation metrics should be accompanied by validation strategies to ensure the reliability of evaluation results. In particular, traditional random cross-validation is often not suitable for academic datasets with an inherent temporal structure, such as semester-based assessments or cumulative student records. Time-aware cross-validation approaches ensure the chronological structure of the data is maintained and prevent information leakage from future observations into the training data. This requirement is critical for maintaining the realism of model evaluation and is thus a common methodological assumption in predictive educational analytics. The principal validation strategies used in educational machine learning research are summarised in Table 1.4. These strategies vary in their suitability for different dataset structures and prediction objectives.

\begin{table}[h]
\caption{Model Validation Strategies in Educational Data Mining}
\label{tab:1.4}
\centering
\begin{tabularx}{\textwidth}{@{} l >{\raggedright\arraybackslash} p{3cm} >{\raggedright\arraybackslash} p{2.2cm} >{\raggedright\arraybackslash} X @{}}
\toprule
\textbf{Strategy} & \textbf{Description} & \textbf{Strengths} & \textbf{Limitations} \\
\midrule
Hold-out split & Single train/test division & Simple \& efficient & Sensitive to split \\
K-fold CV & Repeated data partitions & Stable estimates & Ignores temporal order \\
Time-based split & Train early, test later & Preserves causality & Needs sufficient data \\
Rolling-origin eval & Expanding window validation & Realistic forecasting & Computationally intensive \\
\bottomrule
\end{tabularx}
\smallskip
\small Source: Author’s compilation based on Bergmeir and Benítez (2012) and Hernández-Orallo et al. (2012).
\end{table}


The validation strategies in Table 1.4 highlight that methodological rigour in educational analytics extends beyond the choice of performance metrics. Time-based data splits and rolling-origin validation approaches are especially appropriate for the task of academic performance prediction as they closely align with the real-world conditions of deploying such predictive models. In practice, the prediction results are needed for future semesters or academic years, while the training data available to the model consists of past observations. Time-based validation methods reduce optimistic bias and better estimate the generalisation performance of the models. Rolling-origin evaluation is especially relevant in the context of academic performance prediction as the data is often collected continuously semester after semester, or year after year. By gradually expanding the training window and evaluating the predictions on the next semester or academic year, it effectively simulates the operational use of such predictive systems. While computationally more expensive than other strategies, it can offer valuable insights into the stability and consistency of model performance over time. Besides quantitative performance evaluation, validation of machine learning models also involves qualitative assessment of model behaviour. The interpretability techniques presented in the previous section are a valuable complement to the evaluation metrics in this sense. If the models with the best predictive performance are found to be using features that make sense from the pedagogical perspective, one can be more confident in their validity. If, however, their performance cannot be explained through known patterns in the data, it can be a sign of overfitting to data artefacts.

To accurately assess evaluation results researchers must analyze numerical evaluation metrics along with result stability across validation folds and model interpretability. This comprehensive approach to performance evaluation is in line with the contemporary standards of responsible use of data-driven methods in academic environments. In the context of this thesis, the evaluation framework also provides the link between the model development process and the potential use of the best-performing models to support academic decisions. The proposed framework provides the criteria against which the different prediction models will be compared in later empirical chapters of the thesis. The emphasis on carefully selected metrics and validation strategies should ensure that the subsequent conclusions from the experimental evaluation are scientifically robust and educationally sound.


\section{SUMMARY OF THEORETICAL FINDINGS}
\label{sec:summary}

Chapter 1 presents the theoretical background, including the related literature, and concepts used for modelling and explaining students’ academic performance in machine learning frameworks. Overall, the literature review and theory presentation revealed that the analysis of academic data, known as Educational Data Mining, is a complex and multi-disciplinary domain that unites concepts from statistics, machine learning, the learning sciences, and ethical data governance. The theoretical framing and conceptualising discussed in Chapter 1 are logically connected and justified the choices in the analysis of the following Chapters.

•	Section 1.1 discussed the characteristics of students’ academic performance datasets and their features. In general, educational data often exhibit a dependency on past events, heterogeneous features, and context-specific interactions. Such a nature of the educational datasets is related to the fact that they commonly integrate features of different data types. At the same time, these data are typically gathered over several semesters. The theoretical review also showed that it is not possible to explain the students’ performance on the sole basis of one or several features. Modelling the students’ academic performance typically requires multivariate modelling to account for different effects of features.

•	Section 1.2 reviewed supervised machine learning approaches for the prediction of students’ academic performance. The analysis revealed that classification models, including Logistic Regression, Random Forest, and Gradient Boosting algorithms, remain at the core of the performance modelling in Education Data Mining. In particular, these approaches are able to account for a variety of relationships between features and prediction targets. The theoretical review also provided a comparison of different types of supervised models. In general, it was shown that there is no best model for all problems, and the choice of model must be determined on a case-by-case basis. In this respect, all the reviewed papers supported the approach of utilising both complex models and simple baselines to account for predictive performance and interpretation.

•	Section 1.3 further discussed the theory behind multivariate and feature-interaction modelling in Education Data Mining. The literature review showed that students’ academic performance is shaped by a number of different interdependent factors. Modelling such interdependencies and interactions is possible through multivariate modelling, which allows for testing the interaction effects of features such as a student’s academic history, learning behaviour, and contextual attributes. The theoretical review also showed that when more than one feature is included in the model, a better performance can be achieved due to a better approximation of the learning process in a realistic setting. In addition to this, this section discussed the general methodological issues related to the feature engineering for educational datasets.

•	Section 1.4 covered the key concepts of Explainable Artificial Intelligence (XAI). The theory review showed that the question of how to interpret and explain the models’ predictions has become a critical task in education. The decision-making process should be understandable, explainable, and justifiable. Explainability methods, such as SHAP or LIME, allow for global or local interpretation of the models and are widely used to support the interpretability of various data analysis. Theoretical results also show that explanations are important because they increase trust in the model and support model validation and error analysis. The methods of XAI also have ethical value, as they can be used to detect biases. In this way, the methods of XAI can be considered as an integral part of the prediction modelling.

•	Section 1.5 covered the theory behind the evaluation metrics and validation strategies for students’ academic performance prediction. The theoretical review showed that given the imbalance of the datasets and the importance of the time factor in educational data, the traditional measures, such as the accuracy score, are insufficient for the datasets at hand. In this setting, several other evaluation metrics, such as precision, recall, F1-score, or the ROC-AUC, are utilised to better understand the classifiers’ performance, especially for the early-warning models. The theoretical discussion also showed that time-aware evaluation strategies, such as the rolling-origin evaluation, should be used to better assess the predictive models’ performance. The evaluation process must be complemented by the interpretation of the results and the explainability.


\noindent\textbf{Conclusions (for Chapter~1).}
In conclusion of this chapter, several general observations can be made on the basis of the theoretical review. The prediction of students’ academic performance is a complex task that requires an integrated analytical framework unifying the supervised machine learning, the model evaluation, and Explainable Artificial Intelligence approaches. The choice of the model should not only be determined by the predictive performance but also by its explainability, stability, and ethical appropriateness. Overall, the literature review and the theory presentation provide a wide support for using recent datasets, open, and documented models, and a range of evaluation metrics for the performance modelling in Education Data Mining. The theoretical background provided in Chapter 1 serves two primary purposes in the work. On the one hand, it justifies the choice of datasets, models, evaluation metrics, and XAI techniques in the subsequent empirical analysis. On the other hand, it provides the conceptual lens for the interpretation of the results of the analysis in relation to the current state of educational research and learning theories. Predictive and explainable modelling approaches are thus united in the problem-solving perspective, as the former allows for making predictions, while the latter offers a possibility to draw educational insights from them. Therefore, the conclusions of Chapter 1 determine the methodological boundaries and the opportunities for the practical implementation in the next chapters. Chapter 2, building on the theoretical considerations, presents the details of the research methodology. In particular, this section discusses the selection of datasets, preprocessing steps, and the experimental setup.