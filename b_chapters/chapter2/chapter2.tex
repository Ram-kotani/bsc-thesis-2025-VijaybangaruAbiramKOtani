\chapter{RESEARCH METHODOLOGY, DATASET DESCRIPTION, AND DATA PREPARATION}
\label{chap:analysis}

Chapter 2 details the research design of this thesis. The present chapter formalises the logical flow of the analytic actions to be taken in this research to structure and organise the empirical work of the thesis and, more specifically, to carry out data transformation from raw student academic datasets to statistical models and analytics outputs of predicted and explainable value. The conceptual modelling of the methodological approach to the research problem presented in Chapter 1 integrates research activities across data collection, preprocessing, supervised machine learning modelling, model validation, and explainable AI into a methodologically consistent research design that would ensure scientific and methodological soundness, transparency, and analytic relevance to the problem under study. The overarching principle followed in the methodological design is a research-and-development–oriented analytic approach in which an entire data-processing and modelling pipeline is implemented and empirically tested. The approach represents a methodological reconciliation of three basic analytics practices – quantitative data analysis, predictive modelling, and interpretability analysis – in the area of educational data mining. This design should, in theory, allow the research to contribute to both methodological understanding and practical applicability by developing and testing a functioning predictive and explainable analytical workflow for the assessment of student academic performance. The methodological orientation of the analytic approach reflects a dual practical objective of the thesis – to provide empirical evidence on academic performance prediction using current machine learning methods and to obtain interpretable insights on factors affecting student performance in an academic setting. The analytics workflow is modelled according to an experimental, data-driven design that starts with a dataset acquisition and validation step and continues with successive steps of structured data preprocessing, feature engineering, model implementation and training, and model evaluation. The same multiple ML models are trained on the same curated datasets to allow result consistency and comparability. The predictive outcomes of these models are analysed using standard classification metrics and further explained using explainability techniques to allow for transparent interpretation of model predictions. This approach allows for systematic performance and explainability comparison across various ML approaches.

The logical articulation of the methodological approach to the research in Chapter 2 closely follows the logic of the scientific process itself, building from conceptual problem statements to experimental validation of research hypotheses. The chosen workflow is logically structured in a set of interconnected stages, or phases, of the research-and-development pipeline that are completed in the order presented in Chapter 2 and that produce various methodologically key artefacts. These artefacts are integrated into the next steps of the pipeline and represent an essential part of its methodological coherence. The structure of the research-and-development workflow is, by and large, aligned with the list of steps presented in the methodological literature as essential components of empirical data science. The workflow stages are listed in the order of execution in the empirical work and are as follows:

•	Dataset acquisition and curation, which consists of the selection of recent publicly available student academic datasets from 2020 to 2023, validation of their data structure, and the assessment of ethical compliance;

•	Data preprocessing and feature engineering, which involves the imputation of missing values, encoding of categorical features, normalisation of numerical features, and construction of new and meaningful predictors;

•	Model implementation, which refers to the development and training of supervised ML models including Logistic Regression, Random Forest, and Gradient Boosting models;

•	Explainability analysis, which consists of the application of SHAP and LIME explainability techniques to global and local model explanation, respectively;

•	Evaluation and validation, which refers to the performance assessment of different models trained on the same datasets with respect to a set of performance metrics and a consistent validation scheme.

The various methodological components that make up the structure of the chosen workflow are, by and large, internally coherent and mutually reinforcing. Both the analytical software tools and the research practices that are to be adopted in the course of the research are selected based on prior research on the use of machine learning methods in educational data mining in which scientific validity is determined by model accuracy, model interpretability, and reproducibility. The overall methodological approach to the study is, in a general sense, quantitative, applied, and comparative. Quantitative in the sense that the analysis workflow that is to be developed in this research will use numerical modelling and statistical metrics for performance evaluation, applied in the sense that a complete analytical pipeline is to be developed that can be used for other similar student academic datasets, and comparative in the sense that the workflow will involve a systematic comparison of performance and interpretability of multiple model families using uniform evaluation criteria. The student academic datasets that are used in this research are recent (2020–2023) open-access student performance datasets that represent the educational context to which the research is applied. Preference for these data is given to such student datasets that incorporate a combination of academic, behavioural, and contextual features of students and are available for several academic periods or terms. The recentness of the datasets is used to address the bias towards older datasets in many earlier research works that reduces the relevance of their findings in the context of modern education. All datasets have been ethically collected and stored and thus pass the basic ethical screening as they do not include any personally identifiable information. To ensure maximum methodological reliability and transparency of this research, all stages of data preparation and modelling that are to be implemented in this research have been programmed using the Python programming language. Standard data analysis libraries, such as Pandas and NumPy, and machine learning libraries, such as Scikit-learn and XGBoost, have been used to ensure the reproducibility and scalability of the analytical pipeline. A code-based approach to research implementation allows for independent verification of the preprocessing steps and feature transformations, as well as the experimental conditions. The use of Python as the main programming language has been further justified by its dominance in both academic and industry research and the wide range of machine learning and explainability libraries that have been developed and tested in an open-source community.

The methodological difference between the exploratory and confirmatory stages of analysis is maintained in this study. Exploratory analysis is used to get a first look at the distributions of data, search for anomalies, and identify relationships between features in the student academic datasets under study. Confirmatory analysis is, in turn, focused on model training and hyperparameter tuning, as well as evaluation. This combination of exploratory and confirmatory approaches to data analysis is a typical methodological practice in applied data science and is justified by the need for robust empirical inference. Model validation is based on a battery of well-known classification performance metrics that are known to be most appropriate to the characteristics of education datasets, which is determined by such factors as class imbalance, misclassification costs, and the high prevalence of multiclass classification problems. Fair comparison of model performance between different modelling approaches is also ensured by using the same train–test split schemes and a uniform validation approach. Where applicable, a specific validation scheme that preserves the temporal ordering of student academic records is adopted to prevent information leakage from future to past data records. Hyperparameter tuning and cross-validation are also applied to minimise overfitting and increase generalisation performance.

In general, the methodological approach to this study represents an integrated, all-in-one approach that combines data science theory, computational practices, and the requirement for explainable analysis into one methodological unit. In addition to serving the task of the empirical verification of the theoretical assumptions on student academic performance assessment discussed in Chapter 1, the methodological design of this research is also expected to demonstrate the practical possibility of a functioning and interpretable ML system for student academic performance prediction. The next sections of this chapter detail each of the methodological components starting with the overall research design and continuing with dataset description and preprocessing and model implementation.


\section{RESEARCH DESIGN AND METHODOLOGICAL FRAMEWORK}
\label{sec:case}

The present state-of-the-art of data-driven education research in general is reflected in the research design of this thesis. The combination of disciplines (EDM, supervised ML, and XAI) is a state-of-the-art configuration and the corresponding research design is one that aligns with the specific analytical challenges and use case of this work. The prediction task on student academic performance from modern, publicly available data that characterises the higher education context post-2020 requires a research design, and by consequence, a methodological framework capable of simultaneously grounding in theoretical learning analytics concepts, methodologically transparent computational modelling, and the interpretability of statistical models. As such, the research design that supports the analytical approach to this problem, is a structured, model-driven, and empirically verifiable analytical pipeline in which the methodological choices are justified by the overall research problem, the data, and the envisaged deployment of results for educational decision-support. From a scientific modelling perspective, this thesis problem can be seen as an example of a complicated classification task, where an assemblage of heterogeneous explanatory variables is to predict non-deterministically related outcomes. Student academic performance prediction is, as such, qualitatively different from earlier assessment approaches based on summarising descriptives or single indicators. Predictive modelling generalises across multiple student data dimensions.

The research design of this work reflects this, in that supervised learning is the adopted analytical approach, through which probabilistic relationships between the provided student characteristics and associated academic outcomes can be estimated. The methodological framework is, as such, not an exercise in either exploratory data analysis or in hypothesis-testing in isolation, but in an applied analytical system for which accuracy, robustness, and interpretability are co-equal measures of evaluation. A methodological consideration that is central to this research design is the contextual and temporal aspects of data selected for modelling. An important portion of earlier work in student performance prediction use legacy data sets, the most well-known of which is the UCI Student Performance data set collected prior to 2010. The use of these older data sets for learning analytics is, even when now outdated, sometimes justified by the longevity and methodological benchmarking, however, the use of such legacy data sets in later years raises legitimate questions concerning external validity. Systems of higher education have structurally changed in many ways in the last decade. Due to increased digitalisation and online delivery, blended learning adoption, the increasing popularity of virtual events, asynchronous teaching, the proliferation of e-services, as well as changed assessment methods, particularly post-2020. As such, the research design for this work prioritises recent datasets in its research design, in particular the Kaggle data set “Predict Students Dropout and Academic Success” and other recent, comparable student performance data sets, published between 2020 and 2023. This choice is methodologically justified since it is essential to draw results about the learning process in educational systems using data that is descriptive of the current environment, rather than learning under specific, historically-fixed, conditions. The overall choices of research design and methodology can be described by three broad terms: quantitative, applied, and comparative. Quantitative analysis is required due to the nature of the data, which is based on numerical and categorical data about students and which relies on statistical methods for learning. The applied dimension is, in turn, an outcome of the applied focus on the construction of an analytical pipeline that can be used or adapted in similar settings with comparable educational datasets. Finally, the comparative element of this work is included through the use of multiple supervised learning models, trained and evaluated on equal footing, which allows for an evaluation of the trade-offs between their complexity, performance, and explainability. The last point is especially important for educational analytics, where the need for interpretable reasoning in high-stakes decision-making can be a limiting factor in adopting complex, predictive methods.

The specific methodological framework of this thesis is based on a modular, sequential research-and-development pipeline, in which data preparation, model training, evaluation, and interpretation form a logical progression of interdependent steps. The pipeline-based design of the methodological framework ensures the internal coherence of the approach and precludes a siloed set of analytical decisions across the empirical chapters. Each step of the pipeline is coupled with other steps due to their output products being passed to the next stage, for instance, the choices regarding preprocessing in data preparation have direct implications for model stability, and the choice of evaluation metrics directly inform the explainability techniques applied in later stages. The proposed dependencies are supported by established best practices of empirical data science and provide support for reproducibility and auditability. Supervised machine learning is chosen as the primary modelling paradigm due to its natural compatibility with labelled educational datasets, in which the expected outcomes (target variables) are explicitly stated. This includes the dropout, continuation of enrolment, or graduation outcome, in the case of this thesis. A variety of complementary classification models are used, including Logistic Regression, Random Forest, and Gradient Boosting. This is not an arbitrary selection of models, as these represent three different families of learning algorithms with their own theoretical properties. Logistic Regression is a linear, probabilistic baseline, with interpretable coefficients by design, while Random Forest and Gradient Boosting are non-linear, ensemble models capable of learning feature interactions and non-linear decision boundaries. The co-inclusion of both linear and ensemble-based models is a deliberate choice for a balanced methodological comparison and reflects the current practices in the educational data mining literature. A final design choice that is implicit in the overall methodological approach of this work is that of class imbalance and evaluation realism. The data on student performance is naturally characterised by an unequal class distribution, with the groups of successful students typically being the majority. The choice to include measures to account for this in the research framework is made by the choice of evaluation metrics that can capture the quality of classification beyond mere accuracy. Precision, recall, F1-score, as well as ROC-AUC metrics are all used to capture different aspects of model performance, especially their ability to detect at-risk students with few false positives. The choice of evaluation design is, as such, also aligned with the educational interventions use case, in which costs of misclassification are asymmetric and predictive ranking is usually more relevant than the choice of binary classification threshold.

The model validation in this work is treated as a methodological necessity, rather than a procedural formality. The chosen data splitting uses stratification to maintain class distributions in both training and testing sets, thus minimising the sampling bias and the risk of over-optimistic performance estimates. Cross-validation is included at the training stage, to provide information on model stability and generalisation to repeated sampling. This is a necessity for educational datasets, where the risk of overfitting is higher due to the potential presence of correlated features and institution-specific artefacts, which could otherwise artificially boost the apparent performance. The validation design, in this work, is thus a critical component of the credibility of the empirical results and allows for meaningful generalisation of the reported results beyond the noise of a given dataset. Interpretability is, by design, an integral part of the analytical process rather than an auxiliary add-on. The use of machine learning models for educational decision-making is increasing, but this also creates problems related to the lack of a model interpretability. This has been observed, in particular, in situations where the outcomes concern student progression, support allocation, or academic advising. As such, the methodological framework incorporates explainable artificial intelligence (XAI) techniques alongside predictive modelling. Global and local explanation methods are selected to provide a mechanistic understanding at the cohort level as well as individual prediction rationales. The dual-level interpretability is a necessity both for methodological validation, as well as for the ethical accountability, and practical usability of the analytical results.

Finally, it is important to note that the research design of this work was also purposefully structured to avoid any conflation of correlation with causation in predictive analytics. While the results of this work do result in models that establish statistically significant correlation between student properties and learning outcomes, these models do not purport to make any causal inferences. The research design is therefore also set up to allow for evidence-based interpretability without the need for causal claims, a choice that adheres to generally accepted standards of practice for learning analytics research. In addition to the careful and responsible communication of results, this choice also safeguards against a possible scope-creep in which overly generalized predictive results may be spuriously translated into pedagogically unsubstantiated prescriptive recommendations. From the viewpoint of academic contribution, the novelty of the research design can be thus be found in the confluence of recent datasets, comparative modelling, and interpretability in a single unified analytical pipeline. Novelty is thus not the result of a new algorithm, but of a principled use of established methods on a contemporary education dataset that is framed within a methodologically justified analytical process. In conclusion, the research design and methodological framework summarized in this chapter offers a theoretically sound and rigorous basis for the empirical work that follows in later chapters. The analytical framework in particular was structured to allow for tight alignment between problem, data, modelling choices, and interpretability constraints as driven by the educational domain. Each analytical pipeline structure and the methodological choices that inform it is thus well-justified from both theoretical and data-driven standpoints.

\section{DATA CLEANING, FEATURE ENGINEERING, AND PRE-PROCESSING PIPELINE}
\label{sec:env}

A foundation for good predictive performance is given by the validity of the preparatory data analysis step (Baker \& Inventado, 2016) \cite{baker2018}. Student academic performance datasets are characteristically heterogeneous, multi-source, and feature a combination of academic, socio-demographic and contextual variables, all of which are gathered through formal educational information systems. In most cases, such datasets are unfit to be directly employed as inputs in machine learning systems for supervised modelling and require some form of structured preparatory data processing. This work follows a pipeline to support the pre-modeling data cleaning and feature engineering steps, in a way that preserves data integrity, consistency, and transparency of the analytical process. The pipeline also places emphasis on the preservation of information in the source data while re-casting it into a representation that is appropriate to predictive modeling and follow-up explainability analysis. From the perspective of research practice, data preparation connects the steps of obtaining educational data with feeding machine learning systems in a form that is operationally consistent, statistically appropriate, and replicable by other stakeholders. While early benchmark papers (such as Recasens et al., 2012) adopted small (UCI Student Performance dataset by Cortez \& Silva, 2008) \cite{cortez2008} and very clean datasets, the characteristics of the Kaggle student dataset Predict Students Dropout and Academic Success dataset after 2020. Their feature space is typically higher in dimensionality, more mixed in data type, sparser in non-null observations, and structurally imbalanced across outcome classes, and thus require different data handling decisions. This chapter discusses the methodological trade-offs associated with dataset preprocessing and justifies the specific design choices used in this work. A guiding design principle for the data preparation pipeline in this thesis is the alignment between preprocessing decisions and the later steps of model training, evaluation, and interpretation. In this design choice, data preparation is not a purely technical step but is conceptually connected to the analytical goals and expectations of this thesis. Each step in the transformation of the input dataset is thus guided by pedagogical, statistical, or methodological justifications. An additional guiding principle is the coherence of the model inputs for machine learning modeling and the later stages of explainability analysis. As such, feature engineering in the data preparation pipeline does not adopt any opaque or complex data synthesis transformations but is designed to support the mapping between model inputs and educationally meaningful academic and behavioural patterns in student progress and outcomes.

The data preparation pipeline used in this thesis is illustrated in Figure 2. The pipeline starts with a sanity check of the data against the expected dataset specification. The dataset is validated for congruence between the features, data types, and outcome values. This initial check defines the analytical scope of this thesis and validates that the outcome variable is consistent with the categories of dropout, enrolled, and graduate, which are self-explanatory as a proxy for learning outcomes in an academic system. Such a categorical outcome is coherent with the majority of modelling approaches used in learning analytics systems, for which classification modeling has a higher uptake in early-warning systems (Baker \& Inventado, 2016) \cite{baker2018}. Validation of the dataset at this stage is important for guarding against potential misalignment between dataset structure and the research scope. The next step of the pipeline is addressing missing data in the dataset. Missing data are very common in student datasets (Kuzilek et al., 2017) \cite{kuzilek2017}. Student records might be naturally or administratively incomplete, having null values for optional questions or reflecting system-level inconsistencies. The mere presence of missing values in a dataset might not be important in itself, but research has shown that failing to address null values, or applying a crude delete missing-value strategy, may introduce bias to the learning model, especially if null values are correlated with academic risk (Kuzilek et al., 2017) \cite{kuzilek2017}. The preprocessing pipeline used in this thesis accounts for controlled missing value handling and makes sure that all observations are utilized in the later analysis without distorting the overall sample distribution. The strategy of addressing missing values aligns with recent research, which has recommended cautious data imputation rather than a priori removal of missing cases.

Yet another critical aspect of the data preparation step is the encoding and alignment of feature types. The student dataset under analysis includes both numerical and categorical variables. Supervised machine learning requires numerical input data. However, the application of numeric encoding for categorical features can create artificial ordinal or scale-based assumptions about the nature of the categorical variable. To mitigate this issue, categorical features in this pipeline are encoded through a one-hot encoding strategy, which will create a larger but semantically sparse feature space. This transformation will preserve categorical information while not imposing numerical and ordinal encoding on the categories themselves. This transformation is common to structured data modeling and is appropriate to tree-based and linear classifiers used in this work (James et al., 2021) \cite{james2021}. Another methodological aspect of preprocessing is related to feature scaling. Feature scaling is not universally required for model training, but logistic regression is sensitive to the scale and variance of numerical features and needs standardized input data. Therefore, numerical variables are rescaled during data preparation in this thesis. The strategy of standardization will ensure that coefficient estimates are meaningful and comparable across input features. This concern is model-specific, as tree-based models are insensitive to the monotonic transformation of features. The preprocessing pipeline accounts for model-specific requirements while retaining a coherent and consistent analytical workflow.

The class distribution in a student academic performance dataset is imbalanced, with the majority of observations typically belonging to the positive (successful) category. In the Kaggle dataset, the distribution of classes in the outcome variable is highly unbalanced, with the graduate class being in the majority. Class imbalance is a characteristic of the data and has to be accounted for during training and evaluation. As such, this pipeline preserves the original distribution of the data, but the training-test split is stratified, which means that both training and testing sets will reflect the actual proportions of the data. This strategy will be important for avoiding biases in performance reporting (Sokolova \& Lapalme, 2009) \cite{sokolova2009}. The splitting of data for training and testing purposes is a critical step that directly impacts the credibility of the empirical findings in this thesis. In the pipeline adopted in this work, a stratified hold-out split is used, where a proportion of observations (configurable by the user, but using 70\% for training and 30\% for testing in this case) is separated for training the predictive models. The remaining observations will be used as a testing set for evaluating model predictions. The modeling system can be designed in practice to use data from existing cohorts for training and apply the predictive models to subsequent years’ student cohorts. Splitting the data ahead of training also ensures that there is no information leakage from the evaluation set into the training process. Feature engineering is an area of the data preparation pipeline where steps could be taken that would impede the interpretation of resulting models. For this reason, this work applies limited feature engineering beyond the data synthesis steps illustrated in this chapter, in a way that preserves the transparency and interpretability of original academic and behavioural features.Feature engineering as a pipeline operation is particularly impactful in earlier student performance datasets such as the UCI Student Performance dataset, where limited features and variables allowed the creation of additional and semantically coherent academic, contextual, and socio-demographic features. The student dataset after 2020 (in this case, the Kaggle dataset) has richer sources of academic performance, academic history, and institutional information, which combined and preclude opaque synthesis decisions. This is because the synthesis could create emergent properties that would detract from the interpretation of the models based on the final, engineered dataset. In other words, an approach that has an impact on prediction would not be useful in a thesis that prioritizes the explainability of predictions. For this reason, the preprocessing pipeline ends with a numerical dataset, in which all features are properly encoded, scaled as necessary, and aligned with the outcome variable as its target. This final dataset is then taken as the only input for the modeling and evaluation steps that follow in Chapter 3. All of the predictive models described in that chapter are trained and validated on this data, ensuring comparability and a reasonable level of internal validity. This pipeline also justifies and documents each transformation applied to the input data in the modeling process.


\section{ETHICAL CONSIDERATIONS AND DATA VALIDITY}
\label{sec:constraints}

Predictive modelling in education is both ethically and methodologically inseparable from the choices regarding data selection, processing, and interpretation. Information on student academic performance is a non-neutral byproduct of institutional, social, and structural factors, which represent learning trajectories rather than fixed traits. Any framework designed to forecast academic outcomes must therefore consider both the ethical and validity issues as an integral part of its methodological proposition. In this thesis, this approach is translated into the integration of ethical principles at every stage of the research design, including dataset selection, preprocessing decisions, evaluation strategy, and the interpretability mechanisms for the generated predictions. The dataset used in the research is sourced from Kaggle platform and published under open-access research license. All the records are anonymised at source and lack personal identifying information such as names, ID numbers, and geographically specific references. The absence of direct personal identifiers means that the data should be in compliance with European data protection principles, including the ones stipulated in the General Data Protection Regulation (GDPR), most notably data minimisation and privacy by design (European Parliament \& Council, 2016). The decision to use openly available data also contributes to ethical transparency by making all preprocessing and modelling decisions visible and reproducible for other researchers. Ethical responsibility in educational analytics, however, also extends to the structural properties of the data. The most immediate and tangible risk of predictive modelling from an ethical perspective is the risk of historical bias amplification. Datasets of student performance reflect inequalities in access to resources, assessment practices, and structures that may be reproduced by the predictive model trained on such data. For this reason, the assessment of data validity and bias awareness are treated in this research as two sides of the same coin. In order to assess the validity of the original dataset before any preprocessing was applied, the distribution of the outcome classes was analysed.

The analysis of the raw class distribution is a necessary step for understanding the statistical and ethical implications of the dataset. Educational outcome variables such as dropout, enrolment continuation, and graduation are usually imbalanced in real-life applications. Such imbalance, in turn, has direct consequences for predictive accuracy, fairness, and interpretability. Without an explicit assessment of class proportions, the predictive models may still be highly accurate but systematically neglect minority outcomes which are of educational interest. From an ethical point of view, the identification of class imbalance is necessary because the misclassification costs are asymmetric. Predicting a low-risk outcome for a student who is actually at risk may have an adverse effect and delay the necessary intervention. For these reasons, the figure serves as a diagnostic rather than a descriptive element in this research and informs further methodological decisions as shown in figure 2.1 below.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{fig_class_distribution.png}
    \caption{Class distribution Academic Outcomes (created by author)}
    \label{fig:class-distribution}
\end{figure}

The bar chart as shown I the above figure 2.1 shows a dominant representation of the “Graduate” class relative to other outcomes, as well as an imbalance between “Dropout” and “Enrolled”. As a representational abstraction, this imbalance reflects the real institutional dynamics but also creates methodological risk. A naïve classification model, optimised only for accuracy, may exploit this to its advantage by learning to bias its predictions in favour of the majority class, and produce high performance metrics while not correctly identifying the more vulnerable groups. In order to guard against such risk, accuracy is deliberately excluded as an evaluation criterion in the research design in favour of macro-averaged performance metrics and ROC-AUC analysis. These decisions, in turn, are not only motivated by methodological considerations but also by ethical ones as they guard against overoptimistic assessment of predictive models and prioritise equitable treatment of all the outcome classes. Data validity, finally, is also impacted by the treatment of missing values and the approach to heterogeneous features. Datasets of student performance, like many educational datasets, often include missing or incomplete observations for a range of reasons, from administrative or recording practices to voluntary submission and system-level inconsistencies. Removing such records indiscriminately may affect some outcome classes disproportionately, and distort both the statistical properties and ethical representativeness of the original dataset. In this research, a controlled set of preprocessing steps was applied in order to retain all the observations while ensuring numerical form factor for supervised learning. These steps, in turn, are specifically designed to not alter the informational structure of the dataset but merely to harmonise its format. Following preprocessing, encoding, and stratified partitioning, a final analytical dataset is produced. The effect of all these transformations on the data structure and validity can be traced and measured by comparing them to the original source. The stability of the class representation, feature scale and distributions, and data volume in the processed dataset is a direct signal of its validity. Another diagnostic instrument which is used for both validity and ethical analysis is the confusion matrix. Unlike aggregate performance metrics, the confusion matrix in supervised learning provides a structured view of the model behaviour across all outcome categories. As a diagnostic instrument, it allows for identification of systematic misclassification patterns which may not be visible in the aggregate performance statistics. In educational analytics, such patterns, in turn, may be used as an indicator of whether the model disproportionately mislabels students at-risk. The decision to use a confusion matrix in this research is also guided by the principles of responsible AI, as it allows the inspection of errors rather than obscures them in an averaged measure. Its inclusion in the model validation framework therefore also signals that the predictive performance is assessed with a prior awareness of its potential educational impact.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{fig_confusion_matrix.png}
    \caption{Confusion matrix with Random Forest (created by author)}
    \label{fig:confusion-matrix-rf}
\end{figure}

The matrix shows a relatively balanced distribution of classification errors across all outcome categories. In particular, no single class is shown to be absorbing the majority of prediction errors. This distribution of correct and incorrect predictions is itself a signal that the preprocessing pipeline and stratified evaluation strategy were successful in guarding against majority class dominance in the model training stage. From a data validity perspective, this suggests that the processed dataset does support a meaningful discrimination between the academic outcome categories. Ethically, the lack of extreme misclassification asymmetry increases the justifiability of using the resulting models in an analytical or advisory role. While the research does not claim that the model should be deployed into an operational system, the results do show that with careful preprocessing and evaluation it is possible to mitigate the risk of systematic bias in predictive educational outcomes. Finally, the interpretability analysis which is carried out on the processed dataset using explainable AI methods contributes to the ethical transparency of this research by allowing an inspection of what are the features which drive predictions.


