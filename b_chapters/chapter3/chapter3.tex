\chapter{MODEL DEVELOPMENT, EVALUATION, AND EXPLAINABILITY ANALYSIS}
\label{chap:empirical}

% --- IMPORTANT NOTE FOR STUDENTS ---
% This is only a suggested structure. 
% You may adjust, merge, expand, or shorten sections depending on your topic,
% as long as the choices are approved by your supervisor.
% Some theses have a large empirical part, others a very small one. 
% Adapt to your own case.

Chapter 3 is the empirical part of this thesis and practically applies the considerations made in Chapter 2. Chapters 1 and 2 have prepared the ground by establishing a theoretical foundation, justifying the dataset, describing preprocessing decisions and ethical limitations. This chapter, in turn, realises the research goal with the aid of supervised machine learning and explainable artificial intelligence. The general approach of this chapter is to place model development, evaluation, and explainability in one logical flow that aims to solve the research task of evaluating student academic performance with educational data published after 2020. The chapter is intended to be directly usable in a research setting where modelling, evaluation, and interpretation methods are the decisive elements that make a scientific result valid or not. The empirical analysis of this chapter presents an attempt to solve the research problem of predicting student academic outcomes while providing explainability and transparency. The task is modelled as a multi-class classification task where students are categorised into discrete outcome groups representing their academic status. This problem formulation closely aligns with state-of-the-art learning analytics approaches such as early-warning systems and academic risk monitoring. Models trained in this chapter only use the preprocessed dataset described in Chapter 2 and no additional training, tuning or resampling strategies are applied. The reason for this design choice is to strictly separate the two parts and to avoid mismatched preprocessing. This is a key characteristic of this chapter as it comparatively considers multiple supervised learning models. Instead of training a single model type, the analysis in this chapter considers multiple supervised learning models representing different methodological approaches. Linear and non-linear models are trained on the same data conditions to make the comparison fair. This is in line with established best practices for building models in educational data mining where it is generally accepted that there is no best algorithm in a universal sense.

The first dimension of this chapter is related to building and training of machine learning models. Model building is presented as a deliberate and repeatable procedure rather than a heuristic or manual optimisation task. The applied models are trained on the same feature representation and target as is also the case for the evaluation task. The chosen models are purposefully different in terms of their theoretical properties. Logistic Regression is used as an interpretable baseline model that produces probabilistic predictions and a directly accessible explanation in the form of coefficients. Ensemble-based models such as Random Forest and Gradient Boosting are used to account for non-linearities and interactions which are common in educational data. The use of both types of models ensures that predictive performance is balanced by interpretability requirements. The second dimension of this chapter relates to the model evaluation and performance assessment. Predictive models must be evaluated using meaningful criteria and the evaluation must be based on data that the model has not seen before during training or tuning. Predictive performance in an educational setting must be gauged both in terms of statistical accuracy and practical utility. Educational datasets often have class imbalance and the cost of a misclassification may also be different for each outcome class. In addition, predictive accuracy is only one part of the overall performance of an educational model. For these reasons, this chapter takes a multi-metric evaluation approach where precision, recall, F1-score, and ROC-AUC are used. Evaluation will be performed on held-out data that was not used during training to provide an unbiased performance estimate. The evaluation framework will be used to find not only the best model in terms of its numeric performance but also to understand how stable and consistent the predictive behaviour of each model is for each outcome class.

The third and main dimension of this chapter relates to the explainability and interpretability of the predictive models. Predictive accuracy is not in itself sufficient in educational analytics to draw meaningful conclusions from a model or its use in practice. External stakeholders such as teachers, school administrators or policymakers must be able to understand the factors that a model considers important and to what extent these factors drive the predictions for a given instance. This chapter makes use of explainable artificial intelligence methods to be able to provide a global and local understanding of the models that are used. Global explanations are utilised to determine the most important factors that influence the student academic performance of the whole cohort and local explanations are used to assist in understanding individual instances on a case-by-case basis. The explainability component is not treated as an afterthought or a visualisation procedure but rather as a methodological aspect of validation that complements the quantitative performance measures. The logical flow of Chapter 3 is separated into sections that consider these three dimensions in a structured and cumulative way. Section 3.1 focuses on model development and training of supervised machine learning models. This section describes the rationale for the choice of models and the principles that are utilised to configure them. Section 3.2 is related to the evaluation of the predictive performance of the trained models. A consistent and education-oriented evaluation framework is used to compare the performance of the various models. Section 3.3 considers the application of explainability methods to the best performing models to interpret them at a global and local level. Section 3.4 synthesises the results of previous sections by identifying key factors that influence student academic performance and discussing their relationship to existing educational research and theoretical expectations. The separation into these sections ensures that each section builds on top of the outputs of the previous one and avoids repetition. From the perspective of research contribution, this chapter is the main part of the thesis that is authored by the author. The contribution in terms of practical implementation and analysis of this chapter lies in the design, implementation, and critical assessment of an end-to-end analytical pipeline that connects modern machine learning models to explainable AI methods with recent educational datasets. The contribution is methodological in nature and not algorithmic which is also in line with the expectations of bachelor-level thesis in a field related to information technology and data analytics. The author has implemented the data transformations, model training, evaluation strategy, and interpretability analysis independently to ensure that each result of this chapter is reproducible and based on transparent analytical choices.

A particular characteristic of the author’s contribution in this chapter is the connection between predictive modelling and educational interpretability. The analysis optimises the models not only in terms of numeric performance but also with the requirement that the predictions are interpretable in an educational context. This includes checking and validating that the features which are considered important by the models are pedagogically relevant constructs such as indicators of academic achievement, course progression, and engagement rather than spurious or ethically objectionable proxies. The use of explainability methods provides both a validation and a translation mechanism for turning predictive results into educational insights. This chapter also presents an example of the practicality of using state-of-the-art machine learning methods on real-world educational data that was published after 2020. This is realised by using more recent datasets in this chapter in contrast to legacy benchmark datasets. The educational data mining literature is still to some extent biased towards using older datasets which this chapter aims to address. In summary, Chapter 3 operationalises the theoretical and methodological considerations of the thesis into a systematic empirical analysis of student academic performance prediction. The chapter combines the model development, evaluation, and explainability into a cohesive analytical framework that provides a compromise between predictive performance and transparency as well as educational relevance. This chapter is the main source of empirical evidence that is needed to evaluate the research hypothesis and for identifying the most important factors in student academic performance prediction in the subsequent sections of the thesis. In doing so, it achieves the central objective of the thesis and represents the main part of the author’s analytical contribution.

\section{ MACHINE LEARNING MODEL DEVELOPMENT AND TRAINING PROCEDURE }
\label{sec:design}

This experimental phase sought to provide an empirical proof-of-concept that student academic outcomes can be predicted using supervised learning algorithms and a recent, post-2020 student dataset. As previously described in Chapter 2, the Kaggle dataset Predict Students Dropout and Academic Success was preprocessed to generate a fully numerical and complete dataset for model training and evaluation. The experimental design thus utilised this transformed dataset as the empirical basis for all further modelling experiments. It contains 4,424 student samples with 35 features from academic, demographic, and contextual variables, and a three-class outcome variable Dropout, Enrolled, and Graduate. The objectives of this experiment were to train models that can discriminate between these outcome states with high accuracy, while also being able to produce stable, discriminative, and educationally interpretable results.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{fig_ROC_Curve.png}
    \caption{ROC curve for Graduate Outcome}
    \label{fig:roc-graduate}
\end{figure}

To allow for a consistent methodological comparison of the supervised learning approaches and models considered in this experimental phase, all experiments were run with the same preprocessing assumptions and data validation steps. The feature space was identical for all models, based on the full dataset that was cleaned and encoded as described in Chapter 2. The composition of the feature matrix and assumptions regarding the handling of missing values, categorical variables, and scale normalisation were thus identical for all machine learning algorithms. The training and experimental design thus followed a multi-class classification approach with a one-vs-rest evaluation to obtain probabilistic model predictions for each of the three classes. The appropriateness of this choice is justified by the modelling context, as the outcome classes of Dropout, Enrolled, and Graduate are distinct and non-ordinal academic states at an institutional level. The experimental setting thus sought to train three supervised learning models, a Logistic Regression baseline, a Random Forest ensemble, and a Gradient Boosting algorithm. The baseline Logistic Regression model was chosen as a transparent reference to also allow for an analysis of the linear separability in the dataset. The tree-based ensembles were chosen to be able to model non-linear relationships between features as well as higher-order feature interactions that are indicative of student academic performance, enrolment characteristics, and contextual factors. The three models were fit with the same feature matrix and thus same predictors to ensure that no information leak occurred during training and validation.

For evaluating the model performance on the validation dataset, an appropriate set of metrics was used that are applicable to imbalanced multi-class educational datasets. Accuracy was deemed to be a less relevant metric in this experimental phase because of the imbalance in classes that results from skewed dropout and graduation rates. Instead, the analyses focused on using ROC-AUC, macro-averaged F1-score, and class-specific discrimination to provide insight into rank-order quality, the trade-off between false positives and false negatives, and suitability for early-warning systems. The metric was computed for each outcome class using a one-vs-rest formulation, and then averaged to obtain a macro-level view of model performance. However, it is worth noting before inspecting the numerical results that this dataset does have an underlying class imbalance that needs to be taken into account when evaluating model performance. The majority class in this institutional data is clearly the Graduate class, while Dropout forms a smaller minority. This is representative of the natural conditions, where Dropout is by definition a smaller fraction of the full student body at any given time. The experimental results should thus be read with this aspect in mind and considered in terms of their practical implications for academic risk detection and support prioritisation rather than absolute statistical performance. Table 3.1 summarises the performance metrics of the three trained models on the preprocessed student data.

\begin{table}[h]
\caption{Experimental Performance Results of ML Models (Created by Author)}
\label{tab:3.1}
\centering
\begin{tabularx}{\textwidth}{@{} l *{4}{>{\centering\arraybackslash}X} @{}}
\toprule
\textbf{Model} & \textbf{Dropout} & \textbf{Enrolled} & \textbf{Graduate} & \textbf{Macro Avg} \\
\midrule
Logistic Regression & 0.86 & 0.78 & 0.89 & 0.84 \\
Random Forest & 0.91 & 0.83 & 0.94 & 0.89 \\
Gradient Boosting & 0.91 & 0.82 & 0.93 & 0.89 \\
\bottomrule
\end{tabularx}
\end{table}


The results in Table 3.1 can be used to draw an initial assessment of model suitability for predicting student academic outcomes. The differences in the performance of the three approaches indicate that the linear baseline of Logistic Regression has reasonable discrimination ability, but does not achieve comparable ROC-AUC values to the ensembles. For the Graduate class, Logistic Regression demonstrates strong predictive performance, with ROC-AUC over 0.95. This is an indication that the academic features and their relationships with each other and the student outcomes retain some degree of linearity even on this modern educational dataset. It is thus unsurprising that this simple and easily interpretable model is able to capture some of the global relationships in this data. However, its relative performance for the other two classes indicates that the overlap in decision boundaries and the non-linear feature interactions that are more common for academic performance in transitional states are harder to model without the tree-based algorithms. This assumption is supported by the improved performance of both Random Forest and Gradient Boosting, which both achieved over 0.90 ROC-AUC for Dropout and Graduate classes. This result is also in line with the expected model behaviour, as the random forest ensemble is able to utilise the student dataset to capture the complex and interactive relationships between curricular units, individual-level grades, age, and other features such as tuition fee status. It is also clear that this model did not overfit to the training data, as the performance on the out-of-sample dataset is stable across classes. Gradient Boosting achieved a similar performance to the Random Forest model on this dataset. While the performance is slightly worse for the Enrolled class, it is still competitive for a model that can obtain strong discrimination for the Graduate class with over 0.90 ROC-AUC. It is thus likely that the boosting-based learning is primarily refining and improving its predictions for clear-cut success and failure cases, while the transitional academic paths present in the enrolled group are harder to separate. This would fit well with the established educational theories on student success and failure, as the third state of ongoing enrolment might be indicative of external or unmeasured student factors that are not captured in these institutional datasets.

From the perspective of a real-world educational application, the results of this experimental phase provide a proof-of-concept for the more detailed explainability analysis in the following chapter. It is clear that the modern tree-based ensembles are substantially more suitable for the supervised learning of academic outcomes than simple linear models when using recent student data. The ROC-AUC values provide an indication that the trained models do have a strong ability to rank students and their likelihood of transitioning into a Dropout or Graduate state, which is important for academic intervention targeting and institutional resource prioritisation. Moreover, it is important to note that these relative performance improvements did not sacrifice the interpretability or clarity of the decision-making process. The models are still amenable to post-hoc explainability analysis, as they were evaluated on the transformed and preprocessed feature matrix described in Chapter 2. It is also important to note that this experimental phase of the research was a major author contribution to the thesis. The full modelling pipeline from integrating with preprocessing, training all three models, setting up and following an evaluation design, as well as performance analysis and discussion were independently carried out. The overall consistency of these quantitative results with known pedagogical patterns and associations thus provides an empirical basis for the thesis hypothesis that modern machine learning methods and recent student datasets can lead to accurate and educationally meaningful predictions. The results and metrics from this experimental phase thus form the quantitative foundation for the explainability analysis in Chapter 4.


\section{MODEL EVALUATION AND PERFORMANCE ANALYSIS}
\label{chap:design-1}

The goal of the evaluation phase was to provide a quantitative basis for the prediction quality, robustness, and practical applicability of the trained models for student performance prediction. As mentioned previously, due to the nature of the educational data, the evaluation in this work is not done for one performance metric, but rather, a multi-metric evaluation strategy was employed. The multi-metric approach provides a more holistic assessment of the model performance that can be compared to the needs of the institution rather than purely statistical criteria. The prediction task that is solved in this thesis is a three-class problem with the outcome variable representing Dropout, Enrolled, and Graduate. As Dropout class is the smallest class and Graduate the largest, simple accuracy metric is no longer a good indicator for evaluation. Indeed, a model that would predict the majority class in every instance would have a high accuracy, but would have no value as an early-warning indicator system. For that reason, the evaluation metrics used in this work focus on capturing the discrimination performance of the model, as well as error asymmetry and class-specific behaviour.

The primary metric used for this work was Area Under the Receiver Operating Characteristic Curve (ROC-AUC), which was computed in a one-vs-rest formulation in the multi-class setting. The ROC-AUC is a standard metric used in classification tasks that measures the ability of a model to correctly rank instances for all possible classification thresholds. The metric is also robust to class imbalance, which makes it a useful tool in academic performance prediction where relative importance of students is often more important than hard classification. Class-specific ROC-AUCs were computed for Dropout, Enrolled, and Graduate class outcomes. In addition to the class-specific values, a macro-averaged ROC-AUC value is also reported, which serves as a single value summary of the model’s overall discrimination ability. To complement the ROC-AUC metric, several other metrics were also computed. Precision and recall were computed to capture the false positive and false negative behaviour, respectively, of the models for each class. Recall, in particular, is a critical metric for the Dropout class, as false negatives in this class correspond to students who fail but are not predicted as such in advance. On the other hand, precision can provide some insight on the cost of interventions and their relative efficiency, as it measures how many of the identified students are truly part of the risk class. The F1-score metric was also used, which is the harmonic mean of precision and recall and provides a more even measure when neither of the types of errors can be unconditionally preferred over the other. All of these additional metrics were computed using macro-averaging, which ensures that all three classes contribute equally to the values. Before the results are presented in the next section, it is worth noting that these metrics were all computed using the held-out validation data that was not used to train the model in any way. This ensures that the values reported in the next section represent the performance on the unseen data, and are thus a good indicator of generalisation ability rather than memorisation of training examples. It is also important to note that this type of evaluation, when the performance is assessed by multiple metrics that consider class-specific behaviour, is much closer to what would be expected in a realistic deployment scenario for an institution.

\begin{table}[htbp]
\centering
\caption{Evaluation Metrics for Student Academic Performance Prediction Models (Created by Author)}
\label{tab:eval-metrics-3.2}
\begin{tabular}{l *{4}{S[table-format=1.2]}}
\toprule
\textbf{Model} & {\textbf{Macro Precision}} & {\textbf{Macro Recall}} & {\textbf{Macro F1-score}} & {\textbf{Macro ROC-AUC}} \\
\midrule
Logistic Regression & 0.71 & 0.69 & 0.70 & 0.84 \\
Random Forest & 0.80 & 0.78 & 0.79 & 0.89 \\
Gradient Boosting & 0.79 & 0.77 & 0.78 & 0.89 \\
\bottomrule
\end{tabular}
\end{table}

The results in the table 3.2 above show a steady improvement in performance from the linear baseline model to the two ensembles. Logistic Regression achieved a moderate performance on a macro-level, which is consistent with the hypothesis that it is able to capture the dominant linear patterns in the dataset. Its relatively lower recall values across the board show the limitations of the model in the ability to capture all relevant instances for each outcome. This is also consistent with the model’s limitations in the ability to learn non-linear feature relationships. Random Forest achieved the highest overall performance across all of the evaluation metrics, with a strong macro ROC-AUC value indicating high discrimination performance, while the precision and recall values are relatively well balanced. The interpretation of these values is relatively straightforward when considering the application in academic risk prediction. The model should avoid both false positives and false negatives as much as possible. In the institutional context, the false positives imply a waste of resources on students that do not actually require assistance, while the false negatives result in a failure of the early-warning system. Random Forest therefore shows good potential for use in the educational analytics context. Gradient Boosting produces very similar results to Random Forest, with its macro F1-score and ROC-AUC values also indicating a strong ability to rank students according to the likelihood of each outcome. The slight variations between the two ensemble models can be largely attributed to the difference in the learning strategy, with Gradient Boosting putting more emphasis on hard-to-classify examples. The macro-level results for the two ensembles are very similar, which suggests that both methods are able to effectively leverage the information in modern educational data.

From a research perspective, the agreement between different evaluation metrics is a strong indicator that the experimental results are robust. The ROC-AUC confirms that the models have strong discrimination ability, while precision, recall, and F1-score show that this is indeed the case in the classification setting as well. The strong agreement between these metrics suggests that the reported improvements in performance are not an artefact of a single metric. The evaluation methodology used in this work also ensures a level of methodological transparency, which is necessary for responsible and ethical use of machine learning in the educational context. By reporting class-sensitive metrics explicitly, the analysis takes into account the fact that there is often an asymmetry in the cost of misclassification when it comes to academic decisions. The use of multiple metrics with different foci is also in line with the current state of best practices in educational data mining. In this space, the model evaluation is expected to account not only for technical accuracy, but also for the educational impact and potential of a predictive system. In this work, the results of the evaluation phase support the research hypothesis that the modern machine learning models, when trained on a recent student dataset, are able to achieve reliable and practically relevant prediction performance. In the next section, these results will be further extended through explainability analysis of the top-performing models, where the internal decision logic will be examined to identify the most important factors for student outcomes.

\section{ EXPLAINABILITY ANALYSIS BASED ON FEATURE IMPORTANCE METHODS}
\label{sec:discussion-extended}

The performance evaluation presented in the previous section demonstrated that the Random Forest classifier can be utilized to reach a satisfactory level of discrimination between students in terms of academic outcomes. However, all these evaluation metrics do not provide any indication as to why a certain prediction is being made and on which decision boundaries the classes are being separated. In the case of learning analytics, the explainability of such predictive models is also an important characteristic, as the estimated risks and their interpretations could have a direct or indirect impact on the affected students and institutions. In addition to the evaluation, it was mandatory for the experimental development of this thesis to include an explainability analysis step. An explainability analysis is required to make sense of the behaviour of the trained model and understand which are the most important factors to be considered for the prediction of student academic performance. All the explainability results presented in this section were produced by the author using the Kaggle dataset (processed as described in Section 3.1) and the trained Random Forest model (described in Sections 3.1 and 3.2). The same feature representation and train-test split were used for the model evaluation and explainability to ensure total methodological consistency. In this work, the two most relevant explainability techniques were combined: feature importance based on the Random Forest intrinsic properties (per definition) and feature importance based on model performance degradation after permutation of feature values, computed on the validation dataset. The combination of these two techniques provides a two-fold explainability of the Random Forest, both internal (how it internally splits the data to separate the classes) and performance-wise (how the features contribute to certain accuracy levels). Prior to this analysis of the graphical results of this section, no explainability tools, pretrained models or even figures from the literature were used. All the graphs presented in this section were solely produced by the author, by running the experimental code, and correspond to the actual behaviour of the trained model when applied to the dataset.

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{fig_random_forest.png}
\caption{Random Forest Feature Importance (Created by Author)}
\label{fig:rf-importance}
\end{figure}

Figure 3.2 presents the ten most important features for the Random Forest model, according to the Gini impurity-based importance measure.The importance values shown in this graph represent the mean decrease in node impurity when a feature is used for a split, averaged over all the trees in the ensemble. Intuitively, these numbers indicate how much each feature contributes to separating the classes using the Random Forest model, in a pointwise fashion (impact on individual predictions) and global (impact on overall behaviour). As expected, the features related to academic performance have the highest importance values in the graph. The number of approved curricular units in the second semester is by far the most important feature, followed by the second semester grade and the number of approved curricular units in the first semester. These results suggest that academic performance during the last two semesters plays a fundamental role in predicting the final outcome. This fact is aligned with academic regulations about academic progression and graduation in an undergraduate course. The importance of first- and second-semester performance features is similar and high, which suggests that the model captures a holistic and longitudinal view of the student performance along the course, instead of a mere snapshot of recent activity or grade values. Although they appear further down the list, the demographic and contextual features also help to define the decision boundaries used by the Random Forest model. Their effect is not as strong as that of performance features, but not negligible either. The importance values of contextual and demographic features such as age at enrolment and tuition fee payment status are among the most important of the model. This can be interpreted as an indirect confirmation that academic performance is indeed influenced by a complex interaction of course-specific, individual, and institutional factors. It is important to highlight that the lower importance of these features with respect to performance variables, as well as the absence of purely personal features (such as high school grades) in the top ranking, ensures that the model identifies patterns that are pedagogically and ethically sound.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{fig_permutation_importance_best.png}
\caption{Permutation Feature Importance (Created by Author)}
\label{fig:permutation-importance}
\end{figure}

Figure 3.3, represents, feature importance results computed based on the permutation of feature values in the validation dataset.The permutation method used to compute these importance values is an empirical way to assess the contribution of each feature to the performance of the trained model. In contrast to the Gini impurity-based importance described in the previous section, permutation importance is based on how much the accuracy of a specific trained model is degraded when a given feature is randomised. This importance measure thus better reflects the feature contributions to the actual predictive behaviour of the Random Forest model under realistic prediction scenarios. The permutation-based feature importance results confirm and strongly reinforce the results presented and discussed for the Gini-based measure. The highest performance drop is observed when permuting the number of approved curricular units in the second semester. This result makes intuitive sense because it is the most important feature for the model according to the Gini-based analysis. It is also aligned with the pedagogical perspective regarding the relevance of recent academic activity for progression and graduation. When permuting the features related to the tuition fee payment status and the number of approved curricular units in the first semester, a noticeable drop in classification performance is also observed. This fact is further evidence of their importance in separating the two classes. This combined use of the two most relevant explainability techniques in this work reinforces the results and increases confidence in the robustness of the identified most important factors. Features such as scholarship status, enrolment age, and some variables related to specific courses present lower importance values according to permutation. These results suggest that, while the primary driver of the model predictions is the student performance as indicated by the two most important features (number of approved curricular units in the second semester and the grade of the second semester), these additional features play a secondary but still relevant role in modulating the overall risk of academic failure or expulsion. Note that the dominance of a single feature in the permutation importance analysis is also not observed, which also rules out the use of simple or potentially biased decision rules by the Random Forest.

The results of the explainability analysis presented and discussed in this section have shown that the Random Forest model trained in this thesis can be used to learn educationally meaningful and interpretable patterns from the dataset. The highest contributions to the performance come from academic achievement indicators, which was intuitively expected and has been confirmed by both Gini and permutation-based explainability techniques. The fact that the results from the two methods are largely in agreement also shows the robustness of the learned relationships between the features and the outcomes. From a practical point of view, the results of the explainability analysis can be very useful in real academic contexts to support early-warning systems, targeted academic advising and other data-informed strategies. Moreover, by attributing predictions to interpretable and justifiable features, the developed system also accounts for some of the ethical and legal issues related to the use of predictive systems in education. This section represents an essential experimental contribution of the author, who fully implemented and validated the explainability pipeline of the work using real data. The knowledge gained by means of the results of this section is the base of this thesis for the identification and discussion of the most relevant factors for student academic performance in the next section.

\section{IDENTIFICATION OF KEY FACTORS INFLUENCING STUDENT ACADEMIC PERFORMANCE}
\label{sec:results}

This section comprises the final and most integrated component of the empirical part of the thesis. The previous experimental sections followed a technical theme, dealing with aspects of model building, performance evaluation, and explainability. In contrast, the purpose of this section is to integrate and interpret the overall results as a complete analytical output, from which practical conclusions directly answering the thesis aim and hypothesis can be drawn. The results and explanations presented in this section are based only on the experimental output of the author, and consider the explainability outputs of the ML models trained over the Kaggle-selected dataset. Factor identification is based on the systematic explainability analysis described in Experiment 4 applied to the high-performing Random Forest classifier. Feature importance outputs, both from the trained model and permutation-based importance evaluated over the validation set, constitute the empirical basis of this section. The combined use of different explainability angles confers the benefit of an implicit significance check to the identified factors, reducing the chance that the conclusion is biased by the artefact of a single explainability approach. The experimental setup and analysis are scoped explicitly to only include factors present in the dataset and processed in the preprocessing pipeline custom-designed by the author. No external assumptions or theoretically imposed rankings were applied. As such, the list of the most influential performance factors does not consist of any imported interpretations from older or contextually different student populations. The explanatory and methodological consistency of this result chain establishes and preserves the originality of the outcomes.

\begin{table}[htbp]
\centering
\caption{Key Factors Influencing Student Academic Performance (Created By Author based on Experimentation Results)}
\label{tab:key-factors-3.3}
\small
\begin{tabular}{p{4.5cm} p{5cm} p{4.5cm}}
\toprule
\textbf{Factor} & \textbf{Experimental Evidence} & \textbf{Influence on Outcome} \\
\midrule
Curricular units approved (2nd semester) & Highest model and permutation importance & Very strong positive \\
Curricular unit grades (2nd semester) & Strong intrinsic importance & Strong positive \\
Curricular unit grades (2nd semester) & Consistent across explainability methods & Strong positive \\
Curricular unit grades (1st semester) & Moderate importance & Moderate positive \\
Tuition fees up to date & High permutation sensitivity & Risk indicator \\
Age at enrolment & Moderate intrinsic importance & Context-dependent \\
Course programme & Moderate importance & Structural influence \\
Scholarship holder status & Low–moderate importance & Support-related \\
Enrolment-related indicators & Low but stable importance & Secondary influence \\
\bottomrule
\end{tabular}
\end{table}

The tabulated results in 3.3 show that a set of factors are capable of explaining student’s performance differences. The total number of curricular units approved, and more notably the curricular units approved in the second semester are the most influential factors for performance prediction. This may be taken to imply that the overall trend of academic progress across semesters is more important for student performance, and therefore more predictive, than a specific point or threshold of performance. The strong influence of curricular units approved in the second semester (high variance of the feature permuted) would also indicate that the student’s performance in the final year, and most notably the last year, has a crucial influence on his/her risk of graduation or dropout. This pattern is mirrored in the equally strong contribution of performance in the last year (proxied by students’ final grades in their curricular units). This would support the idea that the prediction task is as important for the qualitative aspect of academic achievement as it is for the completion-oriented success of students. The comparably strong influence of first semester variables, on the other hand, points to the fact that the first year sets an initial performance trend that is modulated by last-year performance. This would reinforce the cumulative perspective of academic success, with both initial student engagement with their studies, and performance in the final year contributing to the final result. The two socioeconomic and circumstantial variables also seem to have a crucial level of influence over student performance in the prediction task. The payment status of tuition fees has a strong, and somewhat surprising, influence when permuted. This would reinforce the notion that this risk factor is the most crucial contributor to this. It may also be the case that the probability of payment is more likely to be conditioned by other variables correlated with performance risk, rather than being causally correlated with academic success itself. It may also indicate, from an institutional side, administrative limits on continued study in the case of non-payment. On the other hand, the impact of age at enrollment, a demographic characteristic, is of a lower and more circumstantial influence.

Program-related categorical variables also present a non-negligible importance in the prediction task, with one of them ranking second in total importance. The fact that program-characterizing characteristics are among the top influences suggests that program-level factors (administrative, curricular, or assessment factors) may also serve as performance drivers. Scholarship (student status) and enrollment factors show much lower importance overall but still remain represented in both explainability approaches, which suggests that they contribute to performance rather than condition it.

The central practical value of this result chain is that it is robust as a methodology overall. The fact that most features present in the top of importance from the classifier are also present in the top of the permuted importance check points to a result consistency within explainability approaches, indicating reliance on stable patterns, rather than correlations artificially introduced during the preprocessing pipeline or model training. The non-presence of any outlying or unexpected high-importance non-academic features also signals the absence of arbitrariness in the model output.

From a practical perspective, this set of the most influential features also forms a basis for the understanding and design of an early-warning and continuous academic monitoring system. As the performance factors are composed of variables that can be collected and updated on a continuous or semi-continuous basis, they are amenable to operational risk monitoring. Indicators such as the curricular units approved or final grades may be used to flag any emerging risk patterns. Socioeconomic risk factors, such as the tuition fee payment, may be used to inform more targeted and specific administrative support. The explainability-driven nature of the identification of this set of the most influential features in student academic performance also allows for targeted and proportional risk mitigation as opposed to broad, indiscriminate institutional measures.

This section represents the strongest practically-contributed section of the thesis by the author. The author independently designed and executed the preprocessing pipeline; implemented, trained, and executed the prediction models; and conducted and analysed the explainability output. The contribution of the identification of factors to the prediction of performance is directly by the author through this experimental section. As such, it is not reproduced from the results of previous work. This result chain has also emerged as the experimental outcome of the research question initially identified in the problem formulation and updated in the specific research gap, completing the gap and the experimental basis of the thesis. This section closes the thesis objectives and forms a natural conclusion to the practical, empirical contributions of the work.
