\chapter{Conclusions}
\label{chap:conclusions}

The Bachelor thesis presented in this work dealt with the problem of predicting students’ academic performance in higher education. To that end, an information technology–based solution was designed and developed. The methodology underpinning the analytical solution draws on a combination of supervised machine learning (ML) and model interpretability methods. The work sets out to contribute to the field in two ways: first, by replicating previous research with a recent and publicly available data set, and second, by providing a reusable, transparent, and data-driven ML pipeline for the prediction of students’ performance that can help with the early detection of students in academic distress. This is different from previous work that used older benchmark data sets and whose modelling solutions were not published.

The first chapter, which provided the basis for the work, performed a theoretical analysis of the research area, which in this case was students’ academic performance. This analysis revealed that the concept was, in fact, a multidimensional construct, determined by three broad categories: academic progression, behavioural engagement, and context-related characteristics. The literature review conducted in the same chapter helped provide evidence to support the two claims set out in the beginning of the work. In the first place, it was confirmed that, to date, supervised classification models are the most common modelling approach in educational data mining (EDM) for the task of students’ academic performance prediction. In the second place, it was noted that model interpretability, transparency, and ethical considerations are an emerging research focus and a critical part of educational predictive analytics. This knowledge and these conclusions also served as the basis for the decision to include explainability in the modelling process.

The methodological choices in the second chapter laid out a reusable, replicable, and reproducible analytical pipeline, which comprises data cleaning and preparation, feature engineering, model training and validation, and the interpretation of the results. The modelling pipeline included deliberate attempts to avoid loss of semantic meaning of the variables in the data set, which had academic connotations, such as courses and subjects, through proper preprocessing. This was also the case with feature extraction and encoding. In a broader sense, the models trained and validated in this work were all compared using the same experimental setting, namely the same data set and the same cross-validation framework. The more recent student data set provided by SenseTime compared to previous research on the same problem of students’ academic performance prediction helped to improve the generalisability of the results to present-day higher education, where conditions are determined, among other things, by digital learning environments and changes to how courses and semesters are structured.

The empirical results in the third chapter showed that ensemble-based ML methods perform the best in predicting students’ academic performance. The Random Forest and the Gradient Boosting classifiers stood out in terms of discrimination capabilities, as confirmed by evaluation metrics that are adequate to imbalanced educational data sets and an early-warning setting. In addition, the results of the model interpretability analysis show that the most important features were related to students’ academic progression. In particular, it was noted that being approved for all curricular units and high grades in all semesters were the most dominant predictors of academic success. These findings were aligned with existing knowledge in the education domain, and it could be concluded that the models were trained on relevant academic variables instead of relying on demographic or proxy features. It was also found that all methods of measuring feature importance gave concordant results, thus strengthening the validity of the conclusions drawn from the model explainability analysis.

The key outcome of the Bachelor thesis work can be summarised as the end-to-end implementation and validation of an accurate and interpretable pipeline for the prediction of student performance using recent student data. In a nutshell, it was confirmed that the goals of predictive accuracy and model interpretability were not conflicting and could be achieved simultaneously without methodological compromises. The analytical approach that was developed, tested, and proven in this work could be used out of the box in practice, e.g. for academic monitoring and early-warning systems, in order to enable informed decision-making and promote student retention. Finally, the work could be built on in different ways, and several research directions were identified for future research, which included the exploration of longitudinal modelling, model fairness-aware evaluation, and the production deployment of predictive models and the corresponding pipelines in actual learning analytics environments.

Limitations of the Research: 
The results of the Bachelor thesis should be seen in the context of certain limitations. First of all, the data set used in the analysis was a single publicly available data set and therefore not necessarily representative of all student populations and other educational systems. In addition, the data set was observational and the analysis performed in this work did not allow for any causal conclusions to be made on any of the predictors included and their relationship with the academic outcomes. The study itself was based on static academic records without other longitudinal or real-time behavioural data. Although an attempt was made to address model interpretability by performing feature importance analysis, this work did not look at fairness or bias for various subgroups.
